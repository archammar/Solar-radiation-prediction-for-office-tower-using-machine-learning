{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the required python libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sb # used to plot the heatmap\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error,  mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "torch.manual_seed(0)\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a python class for loading the data into Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, data, cat_cols=None, output_col=None):\n",
    "        \"\"\"\n",
    "        Characterizes a Dataset for PyTorch\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        data: pandas data frame\n",
    "        The data frame object for the input data. It must\n",
    "        contain all the continuous, categorical and the\n",
    "        output columns to be used.\n",
    "\n",
    "        cat_cols: List of strings\n",
    "        The names of the categorical columns in the data.\n",
    "        These columns will be passed through the embedding\n",
    "        layers in the model. These columns must be\n",
    "        label encoded beforehand. \n",
    "\n",
    "        output_col: string\n",
    "        The name of the output variable column in the data\n",
    "        provided.\n",
    "        \"\"\"\n",
    "\n",
    "        self.n = data.shape[0] # n stores the total number of data points\n",
    "\n",
    "        if output_col:\n",
    "            self.y = data[output_col].astype(np.float32).values.reshape(-1, 1) #data[output_col] contains two columns \n",
    "                                                                            #(first column is the index). This line converts it \n",
    "                                                                            # to single column\n",
    "            \n",
    "        else:\n",
    "            self.y = np.zeros((self.n, 1)) # a general case when out\n",
    "\n",
    "        self.cat_cols = cat_cols if cat_cols else [] # categorical inputs are stored in \"self.cat_cols\"\n",
    "        self.cont_cols = [\n",
    "            col for col in data.columns if col not in self.cat_cols + [output_col] # finds out the columns that have \n",
    "                                                                                   # contionuos inputs\n",
    "        ]\n",
    "\n",
    "        if self.cont_cols:\n",
    "            self.cont_X = data[self.cont_cols].astype(np.float32).values # The data corresponding to \n",
    "                                                                         # continuous columns are stored in \"self.cont_x\"\n",
    "        else:\n",
    "            self.cont_X = np.zeros((self.n, 1))\n",
    "\n",
    "        if self.cat_cols:\n",
    "            self.cat_X = data[cat_cols].astype(np.int64).values # The data corresponding to \n",
    "                                                                # categorical columns are stored in \"self.cont_x\"\n",
    "        else:\n",
    "            self.cat_X = np.zeros((self.n, 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Denotes the total number of samples.\n",
    "        \"\"\"\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Generates one sample of data as a triplet --> #(output, continous inputs, categorical inputs)\n",
    "        \"\"\"\n",
    "        return [self.y[idx], self.cont_X[idx], self.cat_X[idx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of the Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_dims,\n",
    "        no_of_cont,\n",
    "        lin_layer_sizes,\n",
    "        output_size,\n",
    "        emb_dropout,\n",
    "        lin_layer_dropouts,\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        emb_dims: List of two element tuples\n",
    "        This list will contain a two element tuple for each\n",
    "        categorical feature. The first element of a tuple will\n",
    "        denote the number of unique values of the categorical\n",
    "        feature. The second element will denote the embedding\n",
    "        dimension to be used for that feature.\n",
    "\n",
    "        no_of_cont: Integer\n",
    "        The number of continuous features in the data.\n",
    "\n",
    "        lin_layer_sizes: List of integers.\n",
    "        The size of each linear layer. The length will be equal\n",
    "        to the total number\n",
    "        of linear layers in the network.\n",
    "\n",
    "        output_size: Integer\n",
    "        The size of the final output.\n",
    "\n",
    "        emb_dropout: Float\n",
    "        The dropout to be used after the embedding layers.\n",
    "\n",
    "        lin_layer_dropouts: List of floats\n",
    "        The dropouts to be used after each linear layer.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layers of categorical inputs\n",
    "        self.emb_layers = nn.ModuleList([nn.Embedding(x, y) for x, y in emb_dims])\n",
    "\n",
    "        no_of_embs = sum([y for x, y in emb_dims])\n",
    "        self.no_of_embs = no_of_embs\n",
    "        self.no_of_cont = no_of_cont\n",
    "\n",
    "        # Linear Layers of the nueral network\n",
    "        first_lin_layer = nn.Linear(\n",
    "            self.no_of_embs + self.no_of_cont, lin_layer_sizes[0]\n",
    "        ) # first layer where data is fed into the network\n",
    "        # Following lines define the hidden layers\n",
    "        self.lin_layers = nn.ModuleList(\n",
    "            [first_lin_layer]\n",
    "            + [\n",
    "                nn.Linear(lin_layer_sizes[i], lin_layer_sizes[i + 1])\n",
    "                for i in range(len(lin_layer_sizes) - 1)\n",
    "            ]\n",
    "        )\n",
    "        # Initializing the weights of neural network layers\n",
    "        for lin_layer in self.lin_layers:\n",
    "            nn.init.kaiming_normal_(lin_layer.weight.data)\n",
    "\n",
    "        # Output Layer\n",
    "        self.output_layer = nn.Linear(lin_layer_sizes[-1], output_size)\n",
    "        nn.init.kaiming_normal_(self.output_layer.weight.data)\n",
    "\n",
    "        # Batch Norm Layers, It is used to normalize the data into the layers to avoid overfitting and to optimize network\n",
    "        self.first_bn_layer = nn.BatchNorm1d(self.no_of_cont)\n",
    "        self.bn_layers = nn.ModuleList(\n",
    "            [nn.BatchNorm1d(size) for size in lin_layer_sizes]\n",
    "        )\n",
    "\n",
    "        # Dropout Layers used as regularization\n",
    "        self.emb_dropout_layer = nn.Dropout(emb_dropout)\n",
    "        self.droput_layers = nn.ModuleList(\n",
    "            [nn.Dropout(size) for size in lin_layer_dropouts]\n",
    "        )\n",
    "\n",
    "    def forward(self, cont_data, cat_data):\n",
    "        # This function defines the mathematical operations of the neural network \n",
    "        if self.no_of_embs != 0: # Defines the embedding layer of categorical inputs\n",
    "            x = [\n",
    "                emb_layer(cat_data[:, i]) for i, emb_layer in enumerate(self.emb_layers)\n",
    "            ]\n",
    "            x = torch.cat(x, 1)\n",
    "            x = self.emb_dropout_layer(x) # dropout regularization of categorical inputs \n",
    "\n",
    "        if self.no_of_cont != 0:\n",
    "            normalized_cont_data = self.first_bn_layer(cont_data) # defines the linear (normal nueral net hidden layer)\n",
    "                                                                  # for continous inputs\n",
    "\n",
    "            if self.no_of_embs != 0:\n",
    "                x = torch.cat([x, normalized_cont_data], 1)\n",
    "            else:\n",
    "                x = normalized_cont_data\n",
    "        # Batch normalization operation is done in this for loop\n",
    "        for lin_layer, dropout_layer, bn_layer in zip( \n",
    "            self.lin_layers, self.droput_layers, self.bn_layers\n",
    "        ):\n",
    "\n",
    "            x = F.relu(lin_layer(x))\n",
    "#             x = F.sigmoid(lin_layer(x))\n",
    "            x = bn_layer(x)\n",
    "            x = dropout_layer(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x # Returns the output of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is read as a pandas data frame\n",
    "data = pd.read_csv(\"finalized_data.csv\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Array that specifies which columns are categorical inputs\n",
    "categorical_features = [\"0\", \"1\", \"2\", \"3\", \"4\",\"5\", \"6\", \"7\", \"8\"] # Except column \"7\", all are label encoded and \"7\" is \n",
    "                                                               # one-hot encoded. \n",
    "# Array that specifies which columns are continuous inputs\n",
    "contnuous_features = [\"9\",\"10\",\"11\"]\n",
    "output_feature = \"12\"  # specifies which column is output\n",
    "\n",
    "\n",
    "temp={}\n",
    "label_encoders = {} \n",
    "# for loop starts for label encoding\n",
    "for i in range(0,len(categorical_features)):\n",
    "    #codes for one-hot encoding\n",
    "    label_encoders[categorical_features[i]] = OneHotEncoder(handle_unknown='ignore')\n",
    "    label_encoders[categorical_features[i]].fit(data[categorical_features[i]].values.reshape(-1,1))\n",
    "    temp[categorical_features[i]]=label_encoders[categorical_features[i]].transform(data[categorical_features[i]].values.reshape(-1,1)).toarray() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1263600, 13)\n",
      "(1263600, 3)\n",
      "(1263600, 3)\n",
      "(1263600, 3)\n",
      "(1263600, 3)\n",
      "(1263600, 4)\n",
      "(1263600, 3)\n",
      "(1263600, 4)\n",
      "(1263600, 21)\n"
     ]
    }
   ],
   "source": [
    "#printing the dimensions of each categorical variables after one-hot encoding\n",
    "for i in range(0,len(temp)):\n",
    "    print(temp[categorical_features[i]].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the one hot encoded features into the data\n",
    "# The new columns will be named from 13 onwards.\n",
    "ind=13\n",
    "new_categorical_features=[]\n",
    "for i in range(0,len(categorical_features)):\n",
    "    for j in range(0,temp[categorical_features[i]].shape[1]):\n",
    "        data[str(ind)] = temp[categorical_features[i]][:,j]\n",
    "        new_categorical_features.append(str(ind)) # Stroing the column identity of newly added one-hot encoded columns as \n",
    "                                                  # categorical input.\n",
    "        ind=ind+1\n",
    "    data=data.drop([categorical_features[i]], axis=1) # The old column is deleted from the data\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '20',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '30',\n",
       " '31',\n",
       " '32',\n",
       " '33',\n",
       " '34',\n",
       " '35',\n",
       " '36',\n",
       " '37',\n",
       " '38',\n",
       " '39',\n",
       " '40',\n",
       " '41',\n",
       " '42',\n",
       " '43',\n",
       " '44',\n",
       " '45',\n",
       " '46',\n",
       " '47',\n",
       " '48',\n",
       " '49',\n",
       " '50',\n",
       " '51',\n",
       " '52',\n",
       " '53',\n",
       " '54',\n",
       " '55',\n",
       " '56',\n",
       " '57',\n",
       " '58',\n",
       " '59',\n",
       " '60',\n",
       " '61',\n",
       " '62',\n",
       " '63',\n",
       " '64',\n",
       " '65',\n",
       " '66',\n",
       " '67',\n",
       " '68',\n",
       " '69']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 9         10         11        12   13   14   15   16   17  \\\n",
      "0        -1.000000 -13.016747   6.399998  0.010556  1.0  0.0  0.0  0.0  0.0   \n",
      "1        -1.000000 -13.016747   7.199998  0.011241  1.0  0.0  0.0  0.0  0.0   \n",
      "2        -1.000000 -13.016747   7.999998  0.012185  1.0  0.0  0.0  0.0  0.0   \n",
      "3        -1.000000 -13.016747   8.799997  0.013634  1.0  0.0  0.0  0.0  0.0   \n",
      "4        -1.000000 -13.016747   9.599998  0.014240  1.0  0.0  0.0  0.0  0.0   \n",
      "...            ...        ...        ...       ...  ...  ...  ...  ...  ...   \n",
      "1263595  12.283293  -3.000003  66.399963  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1263596  12.283293  -3.000003  67.199959  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1263597  12.283293  -3.000003  67.999962  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1263598  12.283293  -3.000003  68.799957  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1263599  12.283293  -3.000003  69.599960  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "          18  ...   60   61   62   63   64   65   66   67   68   69  \n",
      "0        0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1        0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2        0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3        0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4        0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "1263595  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "1263596  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "1263597  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "1263598  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "1263599  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "\n",
      "[1263600 rows x 61 columns]\n",
      "(1263600, 61)\n"
     ]
    }
   ],
   "source": [
    "print(data) # Data after one-hot encoding.\n",
    "print(data.shape)     # New dimension of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20',\n",
       "       '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32',\n",
       "       '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44',\n",
       "       '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56',\n",
       "       '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68',\n",
       "       '69'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1010875, 61)\n",
      "(1010875, 61)\n",
      "(252725, 61)\n"
     ]
    }
   ],
   "source": [
    "X=data.to_numpy() \n",
    "instances=X.shape[0]/25\n",
    "index = np.random.permutation(int(instances))\n",
    "train=np.zeros((int(np.floor(0.8*instances))*25, X.shape[1]))\n",
    "print(train.shape)\n",
    " \n",
    "\n",
    "for i in range(int(np.floor(0.8*instances))):\n",
    "    t1=i*25\n",
    "    t2=int(index[i]*25)\n",
    "    train[t1:t1+25,:] = X[t2:t2+25,:]\n",
    " \n",
    "\n",
    "print(train.shape)\n",
    "t4=int(instances)-int(np.floor(0.8*instances))\n",
    "test_new=np.zeros((t4*25, data.shape[1]))\n",
    " \n",
    "t3=int(np.floor(0.8*instances))\n",
    "\n",
    "for i in range(t4):\n",
    "    t1=i*25\n",
    "    t2=int(index[i+t3]*25)\n",
    "    test_new[t1:t1+25,:] = X[t2:t2+25,:]\n",
    " \n",
    "print(test_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_permuted=np.zeros((train.shape[0], train.shape[1]))\n",
    "\n",
    "# instances1=train.shape[0]\n",
    "# index1 = np.random.permutation(int(instances1))\n",
    "# for i in range(instances1):\n",
    "#     train_permuted[i,:]=train[int(index1[i]),:]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training (80%) and testing (20%)\n",
    "# train_all, test = train_test_split(data, test_size=0.2, shuffle=True) \n",
    "# Splitting the training data into training set (7/8 portion) into training set and validation set (1/8 portion)\n",
    "val, test = train_test_split(test_new, test_size=2/3, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1010875, 61)\n",
      "(252725, 61)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "# print(val.shape)\n",
    "print(test_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_model(arch, emb_dims, dropout_rate, no_of_epochs, criterion, lr, traindataloader, valdataloader, testdataloader, ground_label, is_test):\n",
    "    device = torch.device('cuda')\n",
    "    model = FeedForwardNN(emb_dims, no_of_cont=3, lin_layer_sizes=arch, output_size=1, emb_dropout=dropout_rate, \n",
    "                          lin_layer_dropouts=[dropout_rate] * len(arch)).to(device)\n",
    "    \n",
    "    no_of_epochs = no_of_epochs # Number of times the data is fed into the network \n",
    "    criterion = criterion # Objective function of the network\n",
    "    lr=lr\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr) # Optimization algorithm to be used\n",
    "    stopper=10 # A value to check overfitting\n",
    "    loss_train_prev=0 # VAriable to store value of objective function in the previous epoch\n",
    "    Ltrain=[] # Python list to store training loss in each epoch\n",
    "    Ltest=[] # Python list to store testing loss in each epoch\n",
    "    \n",
    "    \n",
    "    # Neural network training starts from here\n",
    "    for epoch in range(no_of_epochs):\n",
    "      loss_var=[]  \n",
    "      for y, cont_x, cat_x in traindataloader: # Loading the training dataset\n",
    "          \n",
    "        cat_x = cat_x.to(device)\n",
    "        cont_x = cont_x.to(device)\n",
    "        y  = y.to(device)\n",
    "\n",
    "        # Forward Pass\n",
    "        preds = model(cont_x, cat_x)\n",
    "        loss = criterion(preds, y)\n",
    "        loss_var.append(loss.item())\n",
    "#         print(loss_var)\n",
    "#         print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "        \n",
    "        # Backward Pass and Optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#       print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "      loss_train=sum(loss_var)/(train.shape[0]/batchsize_train) # Finding out the objective function\n",
    "       \n",
    "    \n",
    "      Ltrain.append(loss_train)\n",
    "      \n",
    "       \n",
    "    \n",
    "      # Doing validation of the Neural network\n",
    "      loss_var=[]  \n",
    "      for y, cont_x, cat_x in valdataloader:\n",
    "        \n",
    "        cat_x = cat_x.to(device)\n",
    "        cont_x = cont_x.to(device)\n",
    "        y  = y.to(device)\n",
    "\n",
    "        # Forward Pass\n",
    "        preds = model(cont_x, cat_x)\n",
    "        loss = criterion(preds, y)\n",
    "        loss_var.append(loss.item())\n",
    "      loss_test=sum(loss_var)/(val.shape[0]/batchsize_val)  # Finding out the objective function\n",
    "       \n",
    "      Ltest.append(loss_test)\n",
    "#         print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "        \n",
    "        # Backward Pass and Optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "      if is_test==False:\n",
    "          if epoch % 20==0:\n",
    "              print('Epoch {}: train loss: {} validation loss: {}'.format(epoch, loss_train, loss_test))\n",
    "      else:\n",
    "          print('Epoch {}: train loss: {} validation loss: {}'.format(epoch, loss_train, loss_test))\n",
    "              \n",
    "      if epoch > stopper and Ltest[-1] > np.mean(Ltest[-(stopper+1):-1]):  # Checking overfitting \n",
    "            print(\"Early Stoppin.....\")\n",
    "            break\n",
    "            \n",
    "    # Testing\n",
    "    predicted=np.zeros((0,1))  # Variable to store neural network prediction\n",
    "    for y, cont_x, cat_x in testdataloader:\n",
    "        cat_x = cat_x.to(device)\n",
    "        cont_x = cont_x.to(device)\n",
    "#         y  = y.to(device)\n",
    "        preds = model(cont_x, cat_x)\n",
    "        preds=preds.cpu()\n",
    "        predicted = np.concatenate((predicted, preds.detach().numpy()),axis=0)\n",
    "    actual = ground_label # Variable storing the actual values of the testing cases\n",
    "    actual=actual.reshape(actual.shape[0],1) # Reshaping for visualization\n",
    "    result=np.concatenate((actual, predicted),axis=1) # Actual value and predicted value is stored in \"result\" variable.\n",
    "    \n",
    "    MSE = mean_squared_error(actual, predicted)\n",
    "    MAE = mean_absolute_error(actual, predicted)\n",
    "    R = r2_score(actual, predicted)\n",
    "    \n",
    "    if is_test:\n",
    "        np.savetxt('NN_result.csv', result, delimiter=',')\n",
    "    print('MSE: {} MAE: {} R2: {}'.format(MSE, MAE, R))\n",
    "    return MSE, MAE, R, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "[64]\n",
      "-------------------------------------------------------\n",
      "TRAIN: [ 202175  202176  202177 ... 1010872 1010873 1010874] TEST: [     0      1      2 ... 202172 202173 202174]\n",
      "Epoch 0: train loss: 0.14084819599357748 validation loss: 0.05410032679677987\n",
      "Epoch 20: train loss: 0.0183098758418259 validation loss: 0.025302254457454213\n",
      "Epoch 40: train loss: 0.015615528470375813 validation loss: 0.022958274411617734\n",
      "Early Stoppin.....\n",
      "MSE: 0.020852052509371048 MAE: 0.08172626226401597 R2: 0.7126692682687437\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [202175 202176 202177 ... 404347 404348 404349]\n",
      "Epoch 0: train loss: 0.1558612559566071 validation loss: 0.04853962752662721\n",
      "Epoch 20: train loss: 0.019023121427779177 validation loss: 0.023289978809532572\n",
      "Epoch 40: train loss: 0.016421285064167068 validation loss: 0.021111445058686812\n",
      "Early Stoppin.....\n",
      "MSE: 0.020520788932359847 MAE: 0.08527277572622367 R2: 0.6984976502758373\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [404350 404351 404352 ... 606522 606523 606524]\n",
      "Epoch 0: train loss: 0.2148494063016878 validation loss: 0.05108405805391367\n",
      "Epoch 20: train loss: 0.018533095597660674 validation loss: 0.02462419259865753\n",
      "Early Stoppin.....\n",
      "MSE: 0.02033668420852788 MAE: 0.08433172246974935 R2: 0.7061572652439363\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [606525 606526 606527 ... 808697 808698 808699]\n",
      "Epoch 0: train loss: 0.1320930120454696 validation loss: 0.05249048183198835\n",
      "Epoch 20: train loss: 0.017499795578548222 validation loss: 0.024501280584296244\n",
      "Early Stoppin.....\n",
      "MSE: 0.01969039146584542 MAE: 0.08163942265748118 R2: 0.7131313773608157\n",
      "TRAIN: [     0      1      2 ... 808697 808698 808699] TEST: [ 808700  808701  808702 ... 1010872 1010873 1010874]\n",
      "Epoch 0: train loss: 0.14446866103976916 validation loss: 0.042523997362519876\n",
      "Epoch 20: train loss: 0.017610624220664 validation loss: 0.022496566695512317\n",
      "Early Stoppin.....\n",
      "MSE: 0.021386952008844815 MAE: 0.08440923553144497 R2: 0.6944697133376596\n",
      "-------------------------------------------------------\n",
      "[128]\n",
      "-------------------------------------------------------\n",
      "TRAIN: [ 202175  202176  202177 ... 1010872 1010873 1010874] TEST: [     0      1      2 ... 202172 202173 202174]\n",
      "Epoch 0: train loss: 0.21702559026428275 validation loss: 0.053767327982626976\n",
      "Epoch 20: train loss: 0.016136418937295362 validation loss: 0.023341232300048968\n",
      "Early Stoppin.....\n",
      "MSE: 0.020427974699265265 MAE: 0.08350604994700399 R2: 0.7185128458942038\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [202175 202176 202177 ... 404347 404348 404349]\n",
      "Epoch 0: train loss: 0.1973143795504475 validation loss: 0.046884305225532566\n",
      "Epoch 20: train loss: 0.016252476779583493 validation loss: 0.020796289804895392\n",
      "Early Stoppin.....\n",
      "MSE: 0.0183041086058248 MAE: 0.07877746460568265 R2: 0.731066297087643\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [404350 404351 404352 ... 606522 606523 606524]\n",
      "Epoch 0: train loss: 0.15745144071988185 validation loss: 0.046028652915456256\n",
      "Epoch 20: train loss: 0.015927301019824477 validation loss: 0.022836111455422935\n",
      "Early Stoppin.....\n",
      "MSE: 0.018791694483259735 MAE: 0.07930618605063115 R2: 0.7284806686752803\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [606525 606526 606527 ... 808697 808698 808699]\n",
      "Epoch 0: train loss: 0.1826123971940266 validation loss: 0.05526263373674917\n",
      "Epoch 20: train loss: 0.017298673631175866 validation loss: 0.024662184025176236\n",
      "Early Stoppin.....\n",
      "MSE: 0.019254504629094404 MAE: 0.08186398670250564 R2: 0.7194817973970151\n",
      "TRAIN: [     0      1      2 ... 808697 808698 808699] TEST: [ 808700  808701  808702 ... 1010872 1010873 1010874]\n",
      "Epoch 0: train loss: 0.2031526221795033 validation loss: 0.04265386571527505\n",
      "Epoch 20: train loss: 0.016640565739406768 validation loss: 0.02233170620242103\n",
      "Early Stoppin.....\n",
      "MSE: 0.020959231312549204 MAE: 0.08581761854448877 R2: 0.7005800570134002\n",
      "-------------------------------------------------------\n",
      "[256]\n",
      "-------------------------------------------------------\n",
      "TRAIN: [ 202175  202176  202177 ... 1010872 1010873 1010874] TEST: [     0      1      2 ... 202172 202173 202174]\n",
      "Epoch 0: train loss: 0.20126976034646277 validation loss: 0.05289112428416971\n",
      "Epoch 20: train loss: 0.015778834082456165 validation loss: 0.023327240048617612\n",
      "Early Stoppin.....\n",
      "MSE: 0.02069945090095308 MAE: 0.0829264338089531 R2: 0.7147720412111385\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [202175 202176 202177 ... 404347 404348 404349]\n",
      "Epoch 0: train loss: 0.22396016359712487 validation loss: 0.048149870555908955\n",
      "Epoch 20: train loss: 0.016170222988909338 validation loss: 0.021456341320251832\n",
      "Early Stoppin.....\n",
      "MSE: 0.018609230947531814 MAE: 0.0787776976765996 R2: 0.7265832772933634\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [404350 404351 404352 ... 606522 606523 606524]\n",
      "Epoch 0: train loss: 0.23154689007671003 validation loss: 0.04647120268496334\n",
      "Epoch 20: train loss: 0.015479685866052941 validation loss: 0.02217740415916091\n",
      "Epoch 40: train loss: 0.013931536476972062 validation loss: 0.02100094597114891\n",
      "Early Stoppin.....\n",
      "MSE: 0.01709058908764665 MAE: 0.07462892027961242 R2: 0.7530597719563175\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [606525 606526 606527 ... 808697 808698 808699]\n",
      "Epoch 0: train loss: 0.3427433474066527 validation loss: 0.05753790373440649\n",
      "Epoch 20: train loss: 0.017036874288089848 validation loss: 0.024724783925492256\n",
      "Epoch 40: train loss: 0.014633361556495505 validation loss: 0.02275578225733804\n",
      "Early Stoppin.....\n",
      "MSE: 0.017764276622742006 MAE: 0.07643935604469487 R2: 0.7411928769528574\n",
      "TRAIN: [     0      1      2 ... 808697 808698 808699] TEST: [ 808700  808701  808702 ... 1010872 1010873 1010874]\n",
      "Epoch 0: train loss: 0.2544894217265556 validation loss: 0.04587264423121194\n",
      "Epoch 20: train loss: 0.017231468366017015 validation loss: 0.02267277204110974\n",
      "Epoch 40: train loss: 0.014700867682492702 validation loss: 0.020863513233231715\n",
      "Early Stoppin.....\n",
      "MSE: 0.01886269005161185 MAE: 0.08055064273762712 R2: 0.730530881805484\n",
      "-------------------------------------------------------\n",
      "[512]\n",
      "-------------------------------------------------------\n",
      "TRAIN: [ 202175  202176  202177 ... 1010872 1010873 1010874] TEST: [     0      1      2 ... 202172 202173 202174]\n",
      "Epoch 0: train loss: 0.44004952251594237 validation loss: 0.05717054998776952\n",
      "Epoch 20: train loss: 0.01624309216203686 validation loss: 0.023355182687767217\n",
      "Early Stoppin.....\n",
      "MSE: 0.02057955281043113 MAE: 0.08408456993390251 R2: 0.7164241762260186\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [202175 202176 202177 ... 404347 404348 404349]\n",
      "Epoch 0: train loss: 0.7392625537484099 validation loss: 0.057654198075904224\n",
      "Epoch 20: train loss: 0.01795084334663928 validation loss: 0.022090507594899077\n",
      "Epoch 40: train loss: 0.014958547074700132 validation loss: 0.02068219859091962\n",
      "Early Stoppin.....\n",
      "MSE: 0.017579734925134666 MAE: 0.0765856637643661 R2: 0.7417091806300986\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [404350 404351 404352 ... 606522 606523 606524]\n",
      "Epoch 0: train loss: 0.4806005641351867 validation loss: 0.050694331374080456\n",
      "Epoch 20: train loss: 0.016609986102284733 validation loss: 0.02315225385007311\n",
      "Early Stoppin.....\n",
      "MSE: 0.01936017521041343 MAE: 0.08360046265710191 R2: 0.7202667469852877\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [606525 606526 606527 ... 808697 808698 808699]\n",
      "Epoch 0: train loss: 0.45822903003506305 validation loss: 0.05653715075650176\n",
      "Epoch 20: train loss: 0.01638861045681739 validation loss: 0.02431620880350715\n",
      "Early Stoppin.....\n",
      "MSE: 0.018482699205906467 MAE: 0.07951388549546902 R2: 0.7307262035369044\n",
      "TRAIN: [     0      1      2 ... 808697 808698 808699] TEST: [ 808700  808701  808702 ... 1010872 1010873 1010874]\n",
      "Epoch 0: train loss: 0.42607856666524374 validation loss: 0.04687877680312415\n",
      "Epoch 20: train loss: 0.016467637798761692 validation loss: 0.022727792105469546\n",
      "Early Stoppin.....\n",
      "MSE: 0.019167006012581207 MAE: 0.08133295783076164 R2: 0.7261834767730866\n",
      "-------------------------------------------------------\n",
      "[1024]\n",
      "-------------------------------------------------------\n",
      "TRAIN: [ 202175  202176  202177 ... 1010872 1010873 1010874] TEST: [     0      1      2 ... 202172 202173 202174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 1.0537080732930262 validation loss: 0.06265593094171071\n",
      "Epoch 20: train loss: 0.019343398713175204 validation loss: 0.02707406054021882\n",
      "Early Stoppin.....\n",
      "MSE: 0.02147789223303557 MAE: 0.08729848628212895 R2: 0.7040455135728354\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [202175 202176 202177 ... 404347 404348 404349]\n",
      "Epoch 0: train loss: 1.0920375721614117 validation loss: 0.061688844725245336\n",
      "Epoch 20: train loss: 0.018039279245277765 validation loss: 0.022098317742347717\n",
      "Early Stoppin.....\n",
      "MSE: 0.01992295461081928 MAE: 0.08707266750716315 R2: 0.7072813502244291\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [404350 404351 404352 ... 606522 606523 606524]\n",
      "Epoch 0: train loss: 1.0047536938184858 validation loss: 0.05766521449216076\n",
      "Epoch 20: train loss: 0.01841093188710718 validation loss: 0.024365238570531862\n",
      "Epoch 40: train loss: 0.015114423856314231 validation loss: 0.021587702125066617\n",
      "Early Stoppin.....\n",
      "MSE: 0.018427552979913886 MAE: 0.08346662762339332 R2: 0.7337421131705663\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [606525 606526 606527 ... 808697 808698 808699]\n",
      "Epoch 0: train loss: 0.9518647003456853 validation loss: 0.06376320422917116\n",
      "Epoch 20: train loss: 0.018335190222645716 validation loss: 0.02599813529580343\n",
      "Epoch 40: train loss: 0.015252136603353403 validation loss: 0.024042899124935024\n",
      "Early Stoppin.....\n",
      "MSE: 0.018138229666114936 MAE: 0.0789253183385604 R2: 0.7357447681800988\n",
      "TRAIN: [     0      1      2 ... 808697 808698 808699] TEST: [ 808700  808701  808702 ... 1010872 1010873 1010874]\n",
      "Epoch 0: train loss: 1.4766258157433163 validation loss: 0.06522218290655339\n",
      "Epoch 20: train loss: 0.019533513546638587 validation loss: 0.02429096767159759\n",
      "Epoch 40: train loss: 0.015982236793785052 validation loss: 0.02179107250126659\n",
      "Early Stoppin.....\n",
      "MSE: 0.02020024627921031 MAE: 0.08621638723594405 R2: 0.7114227855477213\n",
      "-------------------------------------------------------\n",
      "[64, 64]\n",
      "-------------------------------------------------------\n",
      "TRAIN: [ 202175  202176  202177 ... 1010872 1010873 1010874] TEST: [     0      1      2 ... 202172 202173 202174]\n",
      "Epoch 0: train loss: 0.22159514634146654 validation loss: 0.06113855574341094\n",
      "Epoch 20: train loss: 0.015251176502024478 validation loss: 0.022304812888996522\n",
      "Epoch 40: train loss: 0.012766412313632769 validation loss: 0.01863886131981357\n",
      "Early Stoppin.....\n",
      "MSE: 0.016964322462589377 MAE: 0.07158831495022205 R2: 0.7662402209897456\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [202175 202176 202177 ... 404347 404348 404349]\n",
      "Epoch 0: train loss: 0.22503378212311897 validation loss: 0.05033683990601633\n",
      "Epoch 20: train loss: 0.015043036315944384 validation loss: 0.02038365449817454\n",
      "Early Stoppin.....\n",
      "MSE: 0.017037958659689108 MAE: 0.07349035118988376 R2: 0.749669245791094\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [404350 404351 404352 ... 606522 606523 606524]\n",
      "Epoch 0: train loss: 0.1728481189184848 validation loss: 0.05400440020517248\n",
      "Epoch 20: train loss: 0.014353344109380614 validation loss: 0.02127785840239681\n",
      "Early Stoppin.....\n",
      "MSE: 0.017677800190147464 MAE: 0.07660298501281851 R2: 0.7445752169291215\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [606525 606526 606527 ... 808697 808698 808699]\n",
      "Epoch 0: train loss: 0.16711804456110568 validation loss: 0.05583687719018733\n",
      "Epoch 20: train loss: 0.014558137502395518 validation loss: 0.019591268258871603\n",
      "Early Stoppin.....\n",
      "MSE: 0.01586183982761341 MAE: 0.07130632400433963 R2: 0.7689094118944451\n",
      "TRAIN: [     0      1      2 ... 808697 808698 808699] TEST: [ 808700  808701  808702 ... 1010872 1010873 1010874]\n",
      "Epoch 0: train loss: 0.17594174612315813 validation loss: 0.044743052149405244\n",
      "Epoch 20: train loss: 0.01432222716811562 validation loss: 0.020616787745327245\n",
      "Early Stoppin.....\n",
      "MSE: 0.016484555415199716 MAE: 0.07103771940969447 R2: 0.7645045007150018\n",
      "-------------------------------------------------------\n",
      "[128, 128]\n",
      "-------------------------------------------------------\n",
      "TRAIN: [ 202175  202176  202177 ... 1010872 1010873 1010874] TEST: [     0      1      2 ... 202172 202173 202174]\n",
      "Epoch 0: train loss: 0.2892378442909804 validation loss: 0.058159218215551534\n",
      "Epoch 20: train loss: 0.013741865077890807 validation loss: 0.01938541523623662\n",
      "Epoch 40: train loss: 0.011784558687131071 validation loss: 0.017195041841048687\n",
      "Early Stoppin.....\n",
      "MSE: 0.01565785704417431 MAE: 0.06838467086079623 R2: 0.7842426533395626\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [202175 202176 202177 ... 404347 404348 404349]\n",
      "Epoch 0: train loss: 0.2803005215203908 validation loss: 0.04934430519332651\n",
      "Epoch 20: train loss: 0.014795570920715671 validation loss: 0.018873096336839628\n",
      "Early Stoppin.....\n",
      "MSE: 0.015945178304161645 MAE: 0.06852499975077715 R2: 0.7657249562225957\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [404350 404351 404352 ... 606522 606523 606524]\n",
      "Epoch 0: train loss: 0.24642540766722018 validation loss: 0.05071201505231076\n",
      "Epoch 20: train loss: 0.01392247736697234 validation loss: 0.020145421221730162\n",
      "Epoch 40: train loss: 0.011955829932631588 validation loss: 0.017220835415188405\n",
      "Early Stoppin.....\n",
      "MSE: 0.014501895636949326 MAE: 0.06493720457190558 R2: 0.790463547084961\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [606525 606526 606527 ... 808697 808698 808699]\n",
      "Epoch 0: train loss: 0.382505170461995 validation loss: 0.05617620875356627\n",
      "Epoch 20: train loss: 0.01630295671755707 validation loss: 0.02357597493368094\n",
      "Epoch 40: train loss: 0.013040824550861214 validation loss: 0.01982421034061518\n",
      "Epoch 60: train loss: 0.011868984277366015 validation loss: 0.01796196107981635\n",
      "Early Stoppin.....\n",
      "MSE: 0.014582528199942275 MAE: 0.0642953244498301 R2: 0.7875476581270248\n",
      "TRAIN: [     0      1      2 ... 808697 808698 808699] TEST: [ 808700  808701  808702 ... 1010872 1010873 1010874]\n",
      "Epoch 0: train loss: 0.2488035112637826 validation loss: 0.047587713135070486\n",
      "Epoch 20: train loss: 0.01370980395541964 validation loss: 0.018785009924017014\n",
      "Epoch 40: train loss: 0.011858701175261858 validation loss: 0.017197522655373713\n",
      "Early Stoppin.....\n",
      "MSE: 0.01518271853409219 MAE: 0.06648592960301691 R2: 0.7831023165845981\n",
      "-------------------------------------------------------\n",
      "[256, 256]\n",
      "-------------------------------------------------------\n",
      "TRAIN: [ 202175  202176  202177 ... 1010872 1010873 1010874] TEST: [     0      1      2 ... 202172 202173 202174]\n",
      "Epoch 0: train loss: 0.6639944388808345 validation loss: 0.06048240451539149\n",
      "Epoch 20: train loss: 0.015554315008750964 validation loss: 0.021694389355109363\n",
      "Epoch 40: train loss: 0.012442012335559235 validation loss: 0.018197377807781346\n",
      "Early Stoppin.....\n",
      "MSE: 0.016237615885052552 MAE: 0.06990054000780697 R2: 0.776253869889955\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [202175 202176 202177 ... 404347 404348 404349]\n",
      "Epoch 0: train loss: 0.5948694308608663 validation loss: 0.05385670914757447\n",
      "Epoch 20: train loss: 0.017219345045130997 validation loss: 0.022043612709299464\n",
      "Epoch 40: train loss: 0.012590720742863513 validation loss: 0.01782356936973138\n",
      "Epoch 60: train loss: 0.011238689916131228 validation loss: 0.01534229730729197\n",
      "Early Stoppin.....\n",
      "MSE: 0.014180576104046307 MAE: 0.06527179862957777 R2: 0.7916514306586845\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [404350 404351 404352 ... 606522 606523 606524]\n",
      "Epoch 0: train loss: 1.2444378624184105 validation loss: 0.0741165580197436\n",
      "Epoch 20: train loss: 0.020106607629118676 validation loss: 0.024311302779395073\n",
      "Epoch 40: train loss: 0.014103239423134602 validation loss: 0.01897401295480181\n",
      "Early Stoppin.....\n",
      "MSE: 0.015785341421463164 MAE: 0.0707034354383819 R2: 0.7719191661344743\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [606525 606526 606527 ... 808697 808698 808699]\n",
      "Epoch 0: train loss: 0.6994481798419407 validation loss: 0.06802804053562586\n",
      "Epoch 20: train loss: 0.01678286436625047 validation loss: 0.023582311835689623\n",
      "Epoch 40: train loss: 0.012848358108895043 validation loss: 0.018915330487318704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stoppin.....\n",
      "MSE: 0.014589307907602359 MAE: 0.06588536337047037 R2: 0.7874488848039185\n",
      "TRAIN: [     0      1      2 ... 808697 808698 808699] TEST: [ 808700  808701  808702 ... 1010872 1010873 1010874]\n",
      "Epoch 0: train loss: 0.48881546280890265 validation loss: 0.0485217880885132\n",
      "Epoch 20: train loss: 0.015026810821650187 validation loss: 0.020518594864206235\n",
      "Epoch 40: train loss: 0.012219631039522864 validation loss: 0.016963756215743354\n",
      "Early Stoppin.....\n",
      "MSE: 0.014937628256801231 MAE: 0.0658878318593137 R2: 0.7866036337731293\n",
      "-------------------------------------------------------\n",
      "[512, 512]\n",
      "-------------------------------------------------------\n",
      "TRAIN: [ 202175  202176  202177 ... 1010872 1010873 1010874] TEST: [     0      1      2 ... 202172 202173 202174]\n",
      "Epoch 0: train loss: 2.5031644453942827 validation loss: 0.09994252697854746\n",
      "Epoch 20: train loss: 0.022578146779907195 validation loss: 0.0294073082384516\n",
      "Epoch 40: train loss: 0.015375266930693467 validation loss: 0.022785905046296902\n",
      "Early Stoppin.....\n",
      "MSE: 0.020420024654614524 MAE: 0.08319323771088855 R2: 0.718622393486498\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [202175 202176 202177 ... 404347 404348 404349]\n",
      "Epoch 0: train loss: 3.904603259531311 validation loss: 0.18604501300170773\n",
      "Epoch 20: train loss: 0.02105700909258542 validation loss: 0.02505698809247525\n",
      "Epoch 40: train loss: 0.015139740006036571 validation loss: 0.019717920495227713\n",
      "Epoch 60: train loss: 0.013101721143079833 validation loss: 0.01750776229701081\n",
      "Early Stoppin.....\n",
      "MSE: 0.017059253711063316 MAE: 0.07636976783590604 R2: 0.7493563675656035\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [404350 404351 404352 ... 606522 606523 606524]\n",
      "Epoch 0: train loss: 4.002405460263796 validation loss: 0.1662669303964396\n",
      "Epoch 20: train loss: 0.02540446539457021 validation loss: 0.03055977439660518\n",
      "Epoch 40: train loss: 0.01595925668622325 validation loss: 0.02242145366722443\n",
      "Epoch 60: train loss: 0.01350902521936033 validation loss: 0.019949842687146585\n",
      "Early Stoppin.....\n",
      "MSE: 0.017019729951484833 MAE: 0.07439985679319616 R2: 0.7540836085925529\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [606525 606526 606527 ... 808697 808698 808699]\n",
      "Epoch 0: train loss: 2.4910759178775783 validation loss: 0.13562928408872885\n",
      "Epoch 20: train loss: 0.02014559468306731 validation loss: 0.027422367366122417\n",
      "Epoch 40: train loss: 0.013842006040412504 validation loss: 0.020523551515624173\n",
      "Early Stoppin.....\n",
      "MSE: 0.01704742868957339 MAE: 0.07670818565657729 R2: 0.7516366093482508\n",
      "TRAIN: [     0      1      2 ... 808697 808698 808699] TEST: [ 808700  808701  808702 ... 1010872 1010873 1010874]\n",
      "Epoch 0: train loss: 2.525763457765151 validation loss: 0.17236078493907803\n",
      "Epoch 20: train loss: 0.01984643629547455 validation loss: 0.025308451630541535\n",
      "Epoch 40: train loss: 0.014531694207982167 validation loss: 0.020296662450447433\n",
      "Early Stoppin.....\n",
      "MSE: 0.01696224170596503 MAE: 0.07363923289229005 R2: 0.7576803572235946\n",
      "-------------------------------------------------------\n",
      "[1024, 1024]\n",
      "-------------------------------------------------------\n",
      "TRAIN: [ 202175  202176  202177 ... 1010872 1010873 1010874] TEST: [     0      1      2 ... 202172 202173 202174]\n",
      "Epoch 0: train loss: 10.941492621500943 validation loss: 0.5915867686271667\n",
      "Epoch 20: train loss: 0.02422470065886119 validation loss: 0.031137742041075816\n",
      "Epoch 40: train loss: 0.017306864033143955 validation loss: 0.02502798588305223\n",
      "Epoch 60: train loss: 0.014795948021252989 validation loss: 0.02136836507830952\n",
      "Early Stoppin.....\n",
      "MSE: 0.01904448508915459 MAE: 0.08071755618854606 R2: 0.7375766326287252\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [202175 202176 202177 ... 404347 404348 404349]\n",
      "Epoch 0: train loss: 9.644655171292106 validation loss: 0.5274318261224715\n",
      "Epoch 20: train loss: 0.03042346036403218 validation loss: 0.03420509593408616\n",
      "Epoch 40: train loss: 0.019392691068954724 validation loss: 0.024591734938201357\n",
      "Epoch 60: train loss: 0.016056952140999618 validation loss: 0.022426853780863714\n",
      "Early Stoppin.....\n",
      "MSE: 0.02196865737643477 MAE: 0.09324895632862719 R2: 0.6772247967116319\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [404350 404351 404352 ... 606522 606523 606524]\n",
      "Epoch 0: train loss: 9.462453825889183 validation loss: 0.5858666378943647\n",
      "Epoch 20: train loss: 0.025767675565821258 validation loss: 0.029584391775434135\n",
      "Epoch 40: train loss: 0.0178669865470123 validation loss: 0.022946439378085683\n",
      "Early Stoppin.....\n",
      "MSE: 0.021231662470112458 MAE: 0.09510419543108375 R2: 0.6932258130349787\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [606525 606526 606527 ... 808697 808698 808699]\n",
      "Epoch 0: train loss: 13.436641799249387 validation loss: 0.7749605833506975\n",
      "Epoch 20: train loss: 0.02736545702841221 validation loss: 0.03407511628064953\n",
      "Epoch 40: train loss: 0.017076597765642704 validation loss: 0.023727130022693853\n",
      "Epoch 60: train loss: 0.013777099876833062 validation loss: 0.019836978864718656\n",
      "Early Stoppin.....\n",
      "MSE: 0.01807717028496527 MAE: 0.08408428647009221 R2: 0.7366343401624532\n",
      "TRAIN: [     0      1      2 ... 808697 808698 808699] TEST: [ 808700  808701  808702 ... 1010872 1010873 1010874]\n",
      "Epoch 0: train loss: 15.562865326957944 validation loss: 0.7345692407889445\n",
      "Epoch 20: train loss: 0.02699008526901711 validation loss: 0.03044005446746701\n",
      "Epoch 40: train loss: 0.019349092456633188 validation loss: 0.025256623773545515\n",
      "Epoch 60: train loss: 0.014724222414128282 validation loss: 0.021309613143322897\n",
      "Early Stoppin.....\n",
      "MSE: 0.018103105925770274 MAE: 0.07742354418429596 R2: 0.7413821688713811\n",
      "-------------------------------------------------------\n",
      "[64, 64, 64]\n",
      "-------------------------------------------------------\n",
      "TRAIN: [ 202175  202176  202177 ... 1010872 1010873 1010874] TEST: [     0      1      2 ... 202172 202173 202174]\n",
      "Epoch 0: train loss: 0.15461782302375304 validation loss: 0.05582853721179923\n",
      "Epoch 20: train loss: 0.013420626025449306 validation loss: 0.01861585209482029\n",
      "Early Stoppin.....\n",
      "MSE: 0.016677113875925867 MAE: 0.07064907554688285 R2: 0.7701978099766542\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [202175 202176 202177 ... 404347 404348 404349]\n",
      "Epoch 0: train loss: 0.21071092988381326 validation loss: 0.05424245379743029\n",
      "Epoch 20: train loss: 0.014558007871321905 validation loss: 0.01846377433995243\n",
      "Early Stoppin.....\n",
      "MSE: 0.018370525149117994 MAE: 0.07676225710564641 R2: 0.7300904698945727\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [404350 404351 404352 ... 606522 606523 606524]\n",
      "Epoch 0: train loss: 0.2022368794423888 validation loss: 0.05595915880603868\n",
      "Epoch 20: train loss: 0.014233343501283814 validation loss: 0.02079805069160266\n",
      "Early Stoppin.....\n",
      "MSE: 0.016444668428758073 MAE: 0.07140401677207137 R2: 0.7623926155456214\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [606525 606526 606527 ... 808697 808698 808699]\n",
      "Epoch 0: train loss: 0.1626545371976258 validation loss: 0.055736130256144725\n",
      "Epoch 20: train loss: 0.01384319099873448 validation loss: 0.018790267499499632\n",
      "Early Stoppin.....\n",
      "MSE: 0.015653781215023175 MAE: 0.06762066462997512 R2: 0.7719406105237618\n",
      "TRAIN: [     0      1      2 ... 808697 808698 808699] TEST: [ 808700  808701  808702 ... 1010872 1010873 1010874]\n",
      "Epoch 0: train loss: 0.1664466866754669 validation loss: 0.045890989881314216\n",
      "Epoch 20: train loss: 0.013958791520755636 validation loss: 0.018463245562476215\n",
      "Early Stoppin.....\n",
      "MSE: 0.016660916498053804 MAE: 0.07043374340930168 R2: 0.7619850368765733\n",
      "-------------------------------------------------------\n",
      "[128, 128, 128]\n",
      "-------------------------------------------------------\n",
      "TRAIN: [ 202175  202176  202177 ... 1010872 1010873 1010874] TEST: [     0      1      2 ... 202172 202173 202174]\n",
      "Epoch 0: train loss: 0.35550514976866565 validation loss: 0.06049206168925176\n",
      "Epoch 20: train loss: 0.014336774445617834 validation loss: 0.02008356981468005\n",
      "Early Stoppin.....\n",
      "MSE: 0.016616468474837245 MAE: 0.07127750017762943 R2: 0.7710334729150196\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [202175 202176 202177 ... 404347 404348 404349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 0.2848469197742721 validation loss: 0.05482194377262084\n",
      "Epoch 20: train loss: 0.014240888813981554 validation loss: 0.01799193235327963\n",
      "Early Stoppin.....\n",
      "MSE: 0.017252627117312576 MAE: 0.07225829580349283 R2: 0.7465152225905982\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [404350 404351 404352 ... 606522 606523 606524]\n",
      "Epoch 0: train loss: 0.4937128441080523 validation loss: 0.06973513453954557\n",
      "Epoch 20: train loss: 0.01508255793223706 validation loss: 0.020196306320731758\n",
      "Early Stoppin.....\n",
      "MSE: 0.015865756136856726 MAE: 0.06897510295210887 R2: 0.7707572618808809\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [606525 606526 606527 ... 808697 808698 808699]\n",
      "Epoch 0: train loss: 0.36123187347087576 validation loss: 0.0600309855319926\n",
      "Epoch 20: train loss: 0.014750892570290884 validation loss: 0.020184738103483545\n",
      "Epoch 40: train loss: 0.012015739668870542 validation loss: 0.018725655637070782\n",
      "Early Stoppin.....\n",
      "MSE: 0.017093661282802076 MAE: 0.0725416549013006 R2: 0.750963048319078\n",
      "TRAIN: [     0      1      2 ... 808697 808698 808699] TEST: [ 808700  808701  808702 ... 1010872 1010873 1010874]\n",
      "Epoch 0: train loss: 0.3206450141698244 validation loss: 0.050513944664939504\n",
      "Epoch 20: train loss: 0.014660086843453572 validation loss: 0.019769061081966417\n",
      "Early Stoppin.....\n",
      "MSE: 0.016841839308331392 MAE: 0.07102430355953382 R2: 0.7594004049914403\n",
      "-------------------------------------------------------\n",
      "[256, 256, 256]\n",
      "-------------------------------------------------------\n",
      "TRAIN: [ 202175  202176  202177 ... 1010872 1010873 1010874] TEST: [     0      1      2 ... 202172 202173 202174]\n",
      "Epoch 0: train loss: 1.2467947251513403 validation loss: 0.09817214859802215\n",
      "Epoch 20: train loss: 0.016636250177390264 validation loss: 0.023092965885508257\n",
      "Epoch 40: train loss: 0.012482786880253536 validation loss: 0.017993736966345153\n",
      "Epoch 60: train loss: 0.011009874546706227 validation loss: 0.016179510499121714\n",
      "Early Stoppin.....\n",
      "MSE: 0.014015492151120449 MAE: 0.06143537226600785 R2: 0.806873610473342\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [202175 202176 202177 ... 404347 404348 404349]\n",
      "Epoch 0: train loss: 1.2320933329597663 validation loss: 0.08911131188029149\n",
      "Epoch 20: train loss: 0.01629452683566836 validation loss: 0.019242281643826454\n",
      "Epoch 40: train loss: 0.012161438539117497 validation loss: 0.015854697399696367\n",
      "Early Stoppin.....\n",
      "MSE: 0.015274787852982129 MAE: 0.06811316018513437 R2: 0.7755746894336093\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [404350 404351 404352 ... 606522 606523 606524]\n",
      "Epoch 0: train loss: 0.9467034198982217 validation loss: 0.11797285177668587\n",
      "Epoch 20: train loss: 0.01578709305887893 validation loss: 0.022133503658849685\n",
      "Epoch 40: train loss: 0.012355831758162582 validation loss: 0.01739332194394264\n",
      "Early Stoppin.....\n",
      "MSE: 0.014659045312822127 MAE: 0.06508613052310637 R2: 0.7881929069918662\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [606525 606526 606527 ... 808697 808698 808699]\n",
      "Epoch 0: train loss: 1.1096585814908924 validation loss: 0.09436729696930432\n",
      "Epoch 20: train loss: 0.01679027485909385 validation loss: 0.021818803165291178\n",
      "Epoch 40: train loss: 0.012456576535562303 validation loss: 0.016846591652538934\n",
      "Early Stoppin.....\n",
      "MSE: 0.014333332410794587 MAE: 0.06474242323787724 R2: 0.7911781828387493\n",
      "TRAIN: [     0      1      2 ... 808697 808698 808699] TEST: [ 808700  808701  808702 ... 1010872 1010873 1010874]\n",
      "Epoch 0: train loss: 1.0442119507349155 validation loss: 0.09303728026933357\n",
      "Epoch 20: train loss: 0.017236258554063563 validation loss: 0.02235442961825699\n",
      "Epoch 40: train loss: 0.012744291465214927 validation loss: 0.017574340822633174\n",
      "Early Stoppin.....\n",
      "MSE: 0.015703897221321123 MAE: 0.06935692277302435 R2: 0.7756568482614119\n",
      "-------------------------------------------------------\n",
      "[512, 512, 512]\n",
      "-------------------------------------------------------\n",
      "TRAIN: [ 202175  202176  202177 ... 1010872 1010873 1010874] TEST: [     0      1      2 ... 202172 202173 202174]\n",
      "Epoch 0: train loss: 5.109351140099284 validation loss: 0.3210103394555264\n",
      "Epoch 20: train loss: 0.025394013244347768 validation loss: 0.032305638962348955\n",
      "Epoch 40: train loss: 0.01635699329939911 validation loss: 0.023387711159274228\n",
      "Early Stoppin.....\n",
      "MSE: 0.022761069426519044 MAE: 0.09447039162121774 R2: 0.6863639811779417\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [202175 202176 202177 ... 404347 404348 404349]\n",
      "Epoch 0: train loss: 4.737701270729764 validation loss: 0.21798654267045317\n",
      "Epoch 20: train loss: 0.019640261566475324 validation loss: 0.024238971901721643\n",
      "Early Stoppin.....\n",
      "MSE: 0.018894837516672962 MAE: 0.08021750004562929 R2: 0.7223869936136009\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [404350 404351 404352 ... 606522 606523 606524]\n",
      "Epoch 0: train loss: 5.139713263296441 validation loss: 0.19054896836398078\n",
      "Epoch 20: train loss: 0.020998172151507295 validation loss: 0.025408946313574664\n",
      "Epoch 40: train loss: 0.014380744822287392 validation loss: 0.01958779885326741\n",
      "Epoch 60: train loss: 0.012388798809322745 validation loss: 0.017544079098667276\n",
      "Early Stoppin.....\n",
      "MSE: 0.01590101966527137 MAE: 0.0724259119565592 R2: 0.7702477426534464\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [606525 606526 606527 ... 808697 808698 808699]\n",
      "Epoch 0: train loss: 4.287492432493472 validation loss: 0.11871057724366423\n",
      "Epoch 20: train loss: 0.022626964273803 validation loss: 0.02977528578800256\n",
      "Epoch 40: train loss: 0.015080247008492424 validation loss: 0.021448214523127822\n",
      "Early Stoppin.....\n",
      "MSE: 0.017016904548655787 MAE: 0.07718347991048395 R2: 0.7520813144866665\n",
      "TRAIN: [     0      1      2 ... 808697 808698 808699] TEST: [ 808700  808701  808702 ... 1010872 1010873 1010874]\n",
      "Epoch 0: train loss: 4.072698091478891 validation loss: 0.17710857704037525\n",
      "Epoch 20: train loss: 0.01920045007361578 validation loss: 0.024751728980756196\n",
      "Epoch 40: train loss: 0.01370632378710382 validation loss: 0.01959547705826212\n",
      "Early Stoppin.....\n",
      "MSE: 0.017741089871368698 MAE: 0.08040986123161532 R2: 0.7465538674300134\n",
      "-------------------------------------------------------\n",
      "[1024, 1024, 1024]\n",
      "-------------------------------------------------------\n",
      "TRAIN: [ 202175  202176  202177 ... 1010872 1010873 1010874] TEST: [     0      1      2 ... 202172 202173 202174]\n",
      "Epoch 0: train loss: 14.05834939663499 validation loss: 0.24057163737836432\n",
      "Epoch 20: train loss: 0.033100485256285166 validation loss: 0.039079878570847826\n",
      "Epoch 40: train loss: 0.01854929865637447 validation loss: 0.026865167390616215\n",
      "Early Stoppin.....\n",
      "MSE: 0.2856046736346412 MAE: 0.4094183601129695 R2: -2.9354878770053205\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [202175 202176 202177 ... 404347 404348 404349]\n",
      "Epoch 0: train loss: 20.57327586174365 validation loss: 0.1937308103823271\n",
      "Epoch 20: train loss: 0.027047858421596443 validation loss: 0.03033160867138964\n",
      "Epoch 40: train loss: 0.017703825277378255 validation loss: 0.02198404638615788\n",
      "Early Stoppin.....\n",
      "MSE: 0.01985089331333142 MAE: 0.08820413519201625 R2: 0.7083401131495961\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [404350 404351 404352 ... 606522 606523 606524]\n",
      "Epoch 0: train loss: 20.16529682650471 validation loss: 0.21339260456991976\n",
      "Epoch 20: train loss: 0.037443563023323446 validation loss: 0.03985518595722855\n",
      "Epoch 40: train loss: 0.019368161284851325 validation loss: 0.02506901995568979\n",
      "Early Stoppin.....\n",
      "MSE: 0.02147847016590581 MAE: 0.08914833749816882 R2: 0.6896597130971949\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [606525 606526 606527 ... 808697 808698 808699]\n",
      "Epoch 0: train loss: 16.972894626361295 validation loss: 0.26189862997805485\n",
      "Epoch 20: train loss: 0.039945848058589944 validation loss: 0.04470106599027993\n",
      "Epoch 40: train loss: 0.018019085719350448 validation loss: 0.024893034707571638\n",
      "Early Stoppin.....\n",
      "MSE: 0.02158913808583074 MAE: 0.09278038291651242 R2: 0.68546860445146\n",
      "TRAIN: [     0      1      2 ... 808697 808698 808699] TEST: [ 808700  808701  808702 ... 1010872 1010873 1010874]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 12.506642347863892 validation loss: 0.33158680210348035\n",
      "Epoch 20: train loss: 0.029439816180687098 validation loss: 0.03200880862528183\n",
      "Early Stoppin.....\n",
      "MSE: 0.038910722545730164 MAE: 0.13618006496023793 R2: 0.44412816708434444\n",
      "-------------------------------------------------------\n",
      "[64, 64, 64, 64]\n",
      "-------------------------------------------------------\n",
      "TRAIN: [ 202175  202176  202177 ... 1010872 1010873 1010874] TEST: [     0      1      2 ... 202172 202173 202174]\n",
      "Epoch 0: train loss: 0.14036800275557088 validation loss: 0.056695678072874664\n",
      "Epoch 20: train loss: 0.013692255675814806 validation loss: 0.020138868848319915\n",
      "Early Stoppin.....\n",
      "MSE: 0.016087697980439308 MAE: 0.06833098956087083 R2: 0.7783196627519663\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [202175 202176 202177 ... 404347 404348 404349]\n",
      "Epoch 0: train loss: 0.20939231847558104 validation loss: 0.0544137395307666\n",
      "Epoch 20: train loss: 0.015169992101373786 validation loss: 0.02016721409363825\n",
      "Early Stoppin.....\n",
      "MSE: 0.01771306088540391 MAE: 0.0751555458013722 R2: 0.7397502846815605\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [404350 404351 404352 ... 606522 606523 606524]\n",
      "Epoch 0: train loss: 0.18772513877922428 validation loss: 0.05357345355460878\n",
      "Epoch 20: train loss: 0.014383356901687466 validation loss: 0.019869769633304876\n",
      "Early Stoppin.....\n",
      "MSE: 0.016218439948479216 MAE: 0.07077999968094566 R2: 0.7656613684378443\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [606525 606526 606527 ... 808697 808698 808699]\n",
      "Epoch 0: train loss: 0.20076077108523513 validation loss: 0.05974873766058781\n",
      "Epoch 20: train loss: 0.013980236633666826 validation loss: 0.019721641907560043\n",
      "Epoch 40: train loss: 0.012202012711377908 validation loss: 0.0182906857187875\n",
      "Early Stoppin.....\n",
      "MSE: 0.015541160340507027 MAE: 0.06754329616319246 R2: 0.7735813800944892\n",
      "TRAIN: [     0      1      2 ... 808697 808698 808699] TEST: [ 808700  808701  808702 ... 1010872 1010873 1010874]\n",
      "Epoch 0: train loss: 0.2023651847825204 validation loss: 0.05271548774765163\n",
      "Epoch 20: train loss: 0.013927069821197936 validation loss: 0.020323742974977025\n",
      "Early Stoppin.....\n",
      "MSE: 0.017650352970307096 MAE: 0.07383669737564431 R2: 0.7478501190595462\n",
      "-------------------------------------------------------\n",
      "[128, 128, 128, 128]\n",
      "-------------------------------------------------------\n",
      "TRAIN: [ 202175  202176  202177 ... 1010872 1010873 1010874] TEST: [     0      1      2 ... 202172 202173 202174]\n",
      "Epoch 0: train loss: 0.27409788096099963 validation loss: 0.057834366214324216\n",
      "Epoch 20: train loss: 0.01349604405445237 validation loss: 0.019588690205309234\n",
      "Early Stoppin.....\n",
      "MSE: 0.019384537538476958 MAE: 0.0803433776839458 R2: 0.7328908819551696\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [202175 202176 202177 ... 404347 404348 404349]\n",
      "Epoch 0: train loss: 0.27717616004703277 validation loss: 0.05324491791304995\n",
      "Epoch 20: train loss: 0.013117925646397362 validation loss: 0.0174606268095677\n",
      "Epoch 40: train loss: 0.011558235318106186 validation loss: 0.015842323939575523\n",
      "Early Stoppin.....\n",
      "MSE: 0.01519209579087794 MAE: 0.06561283230560891 R2: 0.7767896452089516\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [404350 404351 404352 ... 606522 606523 606524]\n",
      "Epoch 0: train loss: 0.31156196879621223 validation loss: 0.059943052955338215\n",
      "Epoch 20: train loss: 0.013948365461845488 validation loss: 0.019922374060652295\n",
      "Epoch 40: train loss: 0.011648636909679207 validation loss: 0.01729514504798123\n",
      "Early Stoppin.....\n",
      "MSE: 0.014520490620657672 MAE: 0.06284526470515789 R2: 0.7901948700081289\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [606525 606526 606527 ... 808697 808698 808699]\n",
      "Epoch 0: train loss: 0.4222138040334344 validation loss: 0.06492048221044854\n",
      "Epoch 20: train loss: 0.014396245685660884 validation loss: 0.02034110269035961\n",
      "Early Stoppin.....\n",
      "MSE: 0.0148166971183986 MAE: 0.06571755989735524 R2: 0.7841360593673457\n",
      "TRAIN: [     0      1      2 ... 808697 808698 808699] TEST: [ 808700  808701  808702 ... 1010872 1010873 1010874]\n",
      "Epoch 0: train loss: 0.4415406048688515 validation loss: 0.05916777985994933\n",
      "Epoch 20: train loss: 0.015477465451154335 validation loss: 0.02194531866517223\n",
      "Early Stoppin.....\n",
      "MSE: 0.01676974488199911 MAE: 0.07045238862270567 R2: 0.7604303334606748\n",
      "-------------------------------------------------------\n",
      "[256, 256, 256, 256]\n",
      "-------------------------------------------------------\n",
      "TRAIN: [ 202175  202176  202177 ... 1010872 1010873 1010874] TEST: [     0      1      2 ... 202172 202173 202174]\n",
      "Epoch 0: train loss: 1.310077267228503 validation loss: 0.13686385616415836\n",
      "Epoch 20: train loss: 0.01690998144610874 validation loss: 0.023298022260919947\n",
      "Epoch 40: train loss: 0.012524999660630153 validation loss: 0.01792030183018231\n",
      "Epoch 60: train loss: 0.010983898301523755 validation loss: 0.016293131059310476\n",
      "Early Stoppin.....\n",
      "MSE: 0.014963802670439362 MAE: 0.06540173218106331 R2: 0.793806371394508\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [202175 202176 202177 ... 404347 404348 404349]\n",
      "Epoch 0: train loss: 1.355348712038578 validation loss: 0.12066671337749137\n",
      "Epoch 20: train loss: 0.017958770624356288 validation loss: 0.021989432300944797\n",
      "Epoch 40: train loss: 0.013169789824303581 validation loss: 0.018000852652504794\n",
      "Early Stoppin.....\n",
      "MSE: 0.01711887067410261 MAE: 0.07296069266399828 R2: 0.7484804434235536\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [404350 404351 404352 ... 606522 606523 606524]\n",
      "Epoch 0: train loss: 1.145040371044466 validation loss: 0.11099938620797924\n",
      "Epoch 20: train loss: 0.01580703943964134 validation loss: 0.022643340232430913\n",
      "Epoch 40: train loss: 0.01238264280741156 validation loss: 0.018489451162883492\n",
      "Early Stoppin.....\n",
      "MSE: 0.015482118667829812 MAE: 0.06809102288330686 R2: 0.7763004016522352\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [606525 606526 606527 ... 808697 808698 808699]\n",
      "Epoch 0: train loss: 1.336153602104805 validation loss: 0.1676180494613335\n",
      "Epoch 20: train loss: 0.015595939142588737 validation loss: 0.023964933806755503\n",
      "Epoch 40: train loss: 0.012616818835948474 validation loss: 0.020039166018488954\n",
      "Early Stoppin.....\n",
      "MSE: 0.015114197347652606 MAE: 0.06783396892143156 R2: 0.7798017889619566\n",
      "TRAIN: [     0      1      2 ... 808697 808698 808699] TEST: [ 808700  808701  808702 ... 1010872 1010873 1010874]\n",
      "Epoch 0: train loss: 1.5500853210558654 validation loss: 0.11633029555688139\n",
      "Epoch 20: train loss: 0.018880485353296495 validation loss: 0.0248322530848081\n",
      "Early Stoppin.....\n",
      "MSE: 0.018324976551256315 MAE: 0.07677430067192363 R2: 0.7382125636009027\n",
      "-------------------------------------------------------\n",
      "[512, 512, 512, 512]\n",
      "-------------------------------------------------------\n",
      "TRAIN: [ 202175  202176  202177 ... 1010872 1010873 1010874] TEST: [     0      1      2 ... 202172 202173 202174]\n",
      "Epoch 0: train loss: 3.601520945413513 validation loss: 0.3008952883423352\n",
      "Epoch 20: train loss: 0.01709316660391764 validation loss: 0.02438299000629636\n",
      "Epoch 40: train loss: 0.012633894567983738 validation loss: 0.018892074690856894\n",
      "Early Stoppin.....\n",
      "MSE: 0.016729274762807923 MAE: 0.07219715388390638 R2: 0.7694790593506026\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [202175 202176 202177 ... 404347 404348 404349]\n",
      "Epoch 0: train loss: 4.831539765857156 validation loss: 0.21792000752003465\n",
      "Epoch 20: train loss: 0.0183092436360623 validation loss: 0.023286768372674457\n",
      "Epoch 40: train loss: 0.01373646853953928 validation loss: 0.01848763722132464\n",
      "Early Stoppin.....\n",
      "MSE: 0.016109752490653155 MAE: 0.07122011611140437 R2: 0.7633069447078003\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [404350 404351 404352 ... 606522 606523 606524]\n",
      "Epoch 0: train loss: 4.1467316826680864 validation loss: 0.1964378381361727\n",
      "Epoch 20: train loss: 0.01951398265159836 validation loss: 0.025789171060333488\n",
      "Epoch 40: train loss: 0.013613190449874687 validation loss: 0.01997566088789799\n",
      "Early Stoppin.....\n",
      "MSE: 0.01684212666352025 MAE: 0.07357738649395407 R2: 0.7566497808998014\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [606525 606526 606527 ... 808697 808698 808699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 4.281535294731954 validation loss: 0.28529774654107015\n",
      "Epoch 20: train loss: 0.016515992754128066 validation loss: 0.022350913860270234\n",
      "Epoch 40: train loss: 0.012518410675602613 validation loss: 0.017625848548944858\n",
      "Early Stoppin.....\n",
      "MSE: 0.014237449272647912 MAE: 0.06344808885396891 R2: 0.7925751009153741\n",
      "TRAIN: [     0      1      2 ... 808697 808698 808699] TEST: [ 808700  808701  808702 ... 1010872 1010873 1010874]\n",
      "Epoch 0: train loss: 2.5358449019149867 validation loss: 0.17132711312809928\n",
      "Epoch 20: train loss: 0.01930039442262146 validation loss: 0.02602007643121188\n",
      "Early Stoppin.....\n",
      "MSE: 0.026645821778550925 MAE: 0.10285790076167003 R2: 0.6193424120002022\n",
      "-------------------------------------------------------\n",
      "[1024, 1024, 1024, 1024]\n",
      "-------------------------------------------------------\n",
      "TRAIN: [ 202175  202176  202177 ... 1010872 1010873 1010874] TEST: [     0      1      2 ... 202172 202173 202174]\n",
      "Epoch 0: train loss: 11.455237609959509 validation loss: 0.21573107897258195\n",
      "Epoch 20: train loss: 0.04962695884633834 validation loss: 0.05512471429881503\n",
      "Epoch 40: train loss: 0.01931498391793068 validation loss: 0.027354750629575528\n",
      "Early Stoppin.....\n",
      "MSE: 0.025867064113374805 MAE: 0.09705875963198961 R2: 0.643564946132035\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [202175 202176 202177 ... 404347 404348 404349]\n",
      "Epoch 0: train loss: 13.584010467963267 validation loss: 0.6009841737200002\n",
      "Epoch 20: train loss: 0.029583405664150804 validation loss: 0.03189227097957838\n",
      "Epoch 40: train loss: 0.018212490092400526 validation loss: 0.023594159938272883\n",
      "Early Stoppin.....\n",
      "MSE: 0.025576171816185315 MAE: 0.10714078187262864 R2: 0.6242212750715136\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [404350 404351 404352 ... 606522 606523 606524]\n",
      "Epoch 0: train loss: 7.7034390632192595 validation loss: 0.12721714902608122\n",
      "Epoch 20: train loss: 0.026572574418776553 validation loss: 0.030742072149134072\n",
      "Epoch 40: train loss: 0.017314539831884863 validation loss: 0.022129181451851228\n",
      "Early Stoppin.....\n",
      "MSE: 0.020241172196999554 MAE: 0.0842615100415884 R2: 0.7075373088332333\n",
      "TRAIN: [      0       1       2 ... 1010872 1010873 1010874] TEST: [606525 606526 606527 ... 808697 808698 808699]\n",
      "Epoch 0: train loss: 13.012696208508094 validation loss: 0.1360726040162024\n",
      "Epoch 20: train loss: 0.04243211061060908 validation loss: 0.0476127879113936\n",
      "Epoch 40: train loss: 0.01850911261081047 validation loss: 0.024129065913987943\n",
      "Early Stoppin.....\n",
      "MSE: 0.022656928400012846 MAE: 0.08985604338487933 R2: 0.6699120048161398\n",
      "TRAIN: [     0      1      2 ... 808697 808698 808699] TEST: [ 808700  808701  808702 ... 1010872 1010873 1010874]\n",
      "Epoch 0: train loss: 8.120642146825997 validation loss: 0.8315602335773531\n",
      "Epoch 20: train loss: 0.04546654396863749 validation loss: 0.04244930410116422\n",
      "Early Stoppin.....\n",
      "MSE: 0.05854325483739467 MAE: 0.1789932986875739 R2: 0.1636612161836628\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "kf = KFold(n_splits)\n",
    "kf.get_n_splits(train) \n",
    "KFold(n_splits=5, random_state=None, shuffle=False)\n",
    "architectures=[ [64], [128], [256], [512], [1024], [64,64], [128,128], [256,256], [512,512], [1024,1024], \n",
    "               [64,64,64], [128,128,128], [256,256,256], [512,512,512], [1024,1024,1024], \n",
    "              [64,64,64,64], [128,128,128,128], [256,256,256,256], [512,512,512,512], [1024,1024,1024,1024],\n",
    "              ]\n",
    "Results = np.zeros((len(architectures), n_splits,3))\n",
    "cat_dims = [2 for col in new_categorical_features]\n",
    "    \n",
    "# Finding out the embedding dimensions for the categorical features\n",
    "emb_dims = [(x, min(50, (x + 1) // 2)) for x in cat_dims]\n",
    "dropout_rate = 0.5\n",
    "no_of_epochs = 100\n",
    "criterion = nn.MSELoss()\n",
    "lr = 0.01\n",
    "k=0\n",
    "for arch in architectures:\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(arch)\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    p=0\n",
    "    for train_index, test_index in kf.split(train):\n",
    "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = train[train_index], train[test_index]\n",
    "        val_k, test_k = train_test_split(X_test, test_size=2/3, shuffle = False)\n",
    "    \n",
    "        train_k=pd.DataFrame(X_train)\n",
    "        val_k=pd.DataFrame(val_k)\n",
    "        test_k=pd.DataFrame(test_k)\n",
    "\n",
    "        train_columns=[]\n",
    "        for i in range(0,61):\n",
    "            train_columns.append(str(i))\n",
    "        train_k.columns=train_columns\n",
    "        test_k.columns=train_columns\n",
    "        val_k.columns=train_columns\n",
    "\n",
    "        new_categorical_features=train_columns[4:]\n",
    "        output_feature=train_columns[3]\n",
    "     \n",
    "        # Creating the training dataset to feed into the network\n",
    "        traindataset = TabularDataset(data=train_k, cat_cols=new_categorical_features,\n",
    "                             output_col=output_feature)\n",
    "        # Creating the validation dataset to feed into the network\n",
    "        valdataset = TabularDataset(data=val_k, cat_cols=new_categorical_features,\n",
    "                             output_col=output_feature)\n",
    "        # Creating the testing dataset to feed into the network\n",
    "        testdataset = TabularDataset(data=test_k, cat_cols=new_categorical_features,\n",
    "                             output_col=output_feature)\n",
    "    \n",
    "        batchsize_train = 16000\n",
    "        batchsize_val=1381\n",
    "        batchsize_test=42121\n",
    "        # Defining pytorch dataloader for train, validation and test dataset\n",
    "        traindataloader = DataLoader(traindataset, batchsize_train, shuffle=True, num_workers=0, ) \n",
    "        valdataloader = DataLoader(valdataset, batchsize_val, shuffle=False, num_workers=0)\n",
    "        testdataloader = DataLoader(testdataset, batchsize_test, shuffle=False, num_workers=0)\n",
    "    \n",
    "        # Finding out the uniue values corresponding to the categoricl features\n",
    "        \n",
    "        MSE, MAE, R, predicted = NN_model(arch, emb_dims, dropout_rate, no_of_epochs, criterion, lr, traindataloader, \n",
    "                               valdataloader, testdataloader, test_k['3'].to_numpy(), is_test = False)\n",
    "         #arch, emb_dims, dropout_rate, no_of_epochs, criterion, lr, traindataloader, valdataloader, testdataloader, \n",
    "            #ground_label, is_test):\n",
    "   \n",
    "        Results[k,p,0] = MSE\n",
    "        Results[k,p,1] = MAE\n",
    "        Results[k,p,2] = R\n",
    "        p+=1\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime of the program is 111349.14245343208\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "\n",
    "# total time taken\n",
    "print(f\"Runtime of the program is {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 5, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture selected through cross validation:\n",
      "[256, 256, 256]\n",
      "Epoch 0: train loss: 1.6486215557231882 validation loss: 0.10508988272459781\n",
      "Epoch 1: train loss: 0.08936936194631068 validation loss: 0.07553083761060825\n",
      "Epoch 2: train loss: 0.07208715063599584 validation loss: 0.06666280530759545\n",
      "Epoch 3: train loss: 0.06566877277846446 validation loss: 0.062299487623767774\n",
      "Epoch 4: train loss: 0.06158097972235427 validation loss: 0.058817513531348745\n",
      "Epoch 5: train loss: 0.05651560457082677 validation loss: 0.05366504650379791\n",
      "Epoch 6: train loss: 0.05056282514928519 validation loss: 0.04861925948472297\n",
      "Epoch 7: train loss: 0.04488806065926965 validation loss: 0.044399455098099394\n",
      "Epoch 8: train loss: 0.04059017216066974 validation loss: 0.04092969055302807\n",
      "Epoch 9: train loss: 0.037134396362116004 validation loss: 0.03803903542336871\n",
      "Epoch 10: train loss: 0.0341938962095583 validation loss: 0.03609708727138941\n",
      "Epoch 11: train loss: 0.03179114661199636 validation loss: 0.034161646285506546\n",
      "Epoch 12: train loss: 0.029907866934696943 validation loss: 0.032741134680929734\n",
      "Epoch 13: train loss: 0.02830329303380599 validation loss: 0.031110151564000082\n",
      "Epoch 14: train loss: 0.026926324912050353 validation loss: 0.02986784144991734\n",
      "Epoch 15: train loss: 0.025745986314879458 validation loss: 0.02912300730460003\n",
      "Epoch 16: train loss: 0.024643442930596803 validation loss: 0.028793810271337383\n",
      "Epoch 17: train loss: 0.02349483670536219 validation loss: 0.02769910950274741\n",
      "Epoch 18: train loss: 0.02285343889873491 validation loss: 0.02657724883346284\n",
      "Epoch 19: train loss: 0.022175833485219953 validation loss: 0.0260249418130175\n",
      "Epoch 20: train loss: 0.021554372182982628 validation loss: 0.02574172961052324\n",
      "Epoch 21: train loss: 0.02097649489962098 validation loss: 0.025207256165439965\n",
      "Epoch 22: train loss: 0.02057399528053409 validation loss: 0.02519831678173581\n",
      "Epoch 23: train loss: 0.019854113244002103 validation loss: 0.02463169069197334\n",
      "Epoch 24: train loss: 0.019577078313958413 validation loss: 0.024246603082560124\n",
      "Epoch 25: train loss: 0.019097426522274873 validation loss: 0.023348763004922477\n",
      "Epoch 26: train loss: 0.01872981436337796 validation loss: 0.023551826319489322\n",
      "Epoch 27: train loss: 0.01833930132901166 validation loss: 0.02241709823796495\n",
      "Epoch 28: train loss: 0.017687258119560723 validation loss: 0.022190193813599525\n",
      "Epoch 29: train loss: 0.017309787277239603 validation loss: 0.021942653265766433\n",
      "Epoch 30: train loss: 0.01699565975837335 validation loss: 0.021837900240035332\n",
      "Epoch 31: train loss: 0.016824467066885944 validation loss: 0.02087050967956664\n",
      "Epoch 32: train loss: 0.016445743944760793 validation loss: 0.02106057150197811\n",
      "Epoch 33: train loss: 0.0164285607961195 validation loss: 0.0206126956818778\n",
      "Epoch 34: train loss: 0.0162746368534149 validation loss: 0.020510774731758188\n",
      "Epoch 35: train loss: 0.015973780997474304 validation loss: 0.021748950659129462\n",
      "Epoch 36: train loss: 0.015923815957544996 validation loss: 0.020642436827059654\n",
      "Epoch 37: train loss: 0.015717969699004415 validation loss: 0.020862101027589352\n",
      "Epoch 38: train loss: 0.01534038837397955 validation loss: 0.020447144361182314\n",
      "Epoch 39: train loss: 0.015551714058695962 validation loss: 0.020304306288112382\n",
      "Epoch 40: train loss: 0.01524637844236168 validation loss: 0.020264531943763864\n",
      "Epoch 41: train loss: 0.015178149587013997 validation loss: 0.019760905902405253\n",
      "Epoch 42: train loss: 0.015087778140488228 validation loss: 0.021327067700932262\n",
      "Early Stoppin.....\n",
      "MSE: 0.015221939930810396 MAE: 0.06868170930825544 R2: 0.760672130368345\n"
     ]
    }
   ],
   "source": [
    "np.savetxt('Cross_validation_MSE_results.csv', Results[:,:,0], delimiter=',')\n",
    "np.savetxt('Cross_validation_MAE_results.csv', Results[:,:,1], delimiter=',')\n",
    "np.savetxt('Cross_validation_R2_results.csv', Results[:,:,2], delimiter=',')\n",
    "RMSE_summary = Results[:,:,0]\n",
    "RMSE_summary = np.sum(RMSE_summary, axis = 1)\n",
    "RMSE_summary = np.argmin(RMSE_summary)\n",
    " \n",
    "arch = architectures[RMSE_summary]\n",
    "print(\"Architecture selected through cross validation:\")\n",
    "print(arch)\n",
    "train=pd.DataFrame(train)\n",
    "val=pd.DataFrame(val)\n",
    "test=pd.DataFrame(test)\n",
    "\n",
    "train_columns=[]\n",
    "for i in range(0,61):\n",
    "    train_columns.append(str(i))\n",
    "train.columns=train_columns\n",
    "test.columns=train_columns\n",
    "val.columns=train_columns\n",
    "\n",
    "new_categorical_features=train_columns[4:]\n",
    "output_feature=train_columns[3]\n",
    "     \n",
    "# Creating the training dataset to feed into the network\n",
    "traindataset = TabularDataset(data=train, cat_cols=new_categorical_features,\n",
    "                             output_col=output_feature)\n",
    "# Creating the validation dataset to feed into the network\n",
    "valdataset = TabularDataset(data=val, cat_cols=new_categorical_features,\n",
    "                             output_col=output_feature)\n",
    "# Creating the testing dataset to feed into the network\n",
    "testdataset = TabularDataset(data=test, cat_cols=new_categorical_features,\n",
    "                             output_col=output_feature)\n",
    "    \n",
    "batchsize_train = 16000\n",
    "batchsize_val=1381\n",
    "batchsize_test=42121\n",
    "# Defining pytorch dataloader for train, validation and test dataset\n",
    "traindataloader = DataLoader(traindataset, batchsize_train, shuffle=True, num_workers=0) \n",
    "valdataloader = DataLoader(valdataset, batchsize_val, shuffle=False, num_workers=0)\n",
    "testdataloader = DataLoader(testdataset, batchsize_test, shuffle=False, num_workers=0)\n",
    "    \n",
    "# Finding out the uniue values corresponding to the categoricl features\n",
    "cat_dims = [2 for col in new_categorical_features]\n",
    "    \n",
    "# Finding out the embedding dimensions for the categorical features\n",
    "emb_dims = [(x, min(50, (x + 1) // 2)) for x in cat_dims]\n",
    "dropout_rate = 0.5\n",
    "no_of_epochs = 100\n",
    "criterion = nn.MSELoss()\n",
    "lr = 0.01\n",
    "actual= test['3'].to_numpy()\n",
    "MSE, MAE, R, predicted = NN_model(arch, emb_dims, dropout_rate, no_of_epochs, criterion, lr, traindataloader, valdataloader, testdataloader, actual,is_test = True)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error is:\n",
      "0.015221939930810396\n",
      "Mean absolute error is:\n",
      "0.06868170930825544\n",
      "R2 score is:\n",
      "0.760672130368345\n"
     ]
    }
   ],
   "source": [
    "# The mean suared error of the prediction is printed\n",
    "\n",
    "print(\"Mean squared error is:\")\n",
    "print(MSE)\n",
    "print(\"Mean absolute error is:\")\n",
    "print(MAE)\n",
    "print(\"R2 score is:\")\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter an integer between 0 to 6738 to test the model : 1237\n",
      "Heatmap Actual and Heatmap predicted\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "actual =  test['3'].to_numpy()\n",
    "actual = np.reshape(actual,(actual.shape[0],1))\n",
    "real=np.zeros((25,1))\n",
    "predctd = np.zeros((25,1))\n",
    "\n",
    "pos = int(input(\"Enter an integer between 0 to 6738 to test the model : \"))\n",
    "pos = pos*25+9\n",
    "\n",
    "real=actual[pos:pos+25,-1]\n",
    "predctd = predicted[pos:pos+25,-1]\n",
    "\n",
    "real=np.reshape(real,(5,5))\n",
    "predctd = np.reshape(predctd,(5,5))\n",
    "\n",
    "\n",
    "print(\"Heatmap Actual and Heatmap predicted\")\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "heat_map = sb.heatmap(real, vmin = 0, vmax = 1.30141, cmap=\"OrRd_r\", annot=True)\n",
    "plt.subplot(122)\n",
    "heat_map = sb.heatmap(predctd,  vmin = 0, vmax = 1.30141, cmap=\"OrRd_r\", annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
