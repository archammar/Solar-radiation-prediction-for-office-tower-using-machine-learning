{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the required python libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sb # used to plot the heatmap\n",
    "import csv\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error,  mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "torch.manual_seed(0)\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a python class for loading the data into Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, data, cat_cols=None, output_col=None):\n",
    "        \"\"\"\n",
    "        Characterizes a Dataset for PyTorch\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        data: pandas data frame\n",
    "        The data frame object for the input data. It must\n",
    "        contain all the continuous, categorical and the\n",
    "        output columns to be used.\n",
    "\n",
    "        cat_cols: List of strings\n",
    "        The names of the categorical columns in the data.\n",
    "        These columns will be passed through the embedding\n",
    "        layers in the model. These columns must be\n",
    "        label encoded beforehand. \n",
    "\n",
    "        output_col: string\n",
    "        The name of the output variable column in the data\n",
    "        provided.\n",
    "        \"\"\"\n",
    "\n",
    "        self.n = data.shape[0] # n stores the total number of data points\n",
    "\n",
    "        if output_col:\n",
    "            self.y = data[output_col].astype(np.float32).values.reshape(-1, 1) #data[output_col] contains two columns \n",
    "                                                                            #(first column is the index). This line converts it \n",
    "                                                                            # to single column\n",
    "            \n",
    "        else:\n",
    "            self.y = np.zeros((self.n, 1)) # a general case when out\n",
    "\n",
    "        self.cat_cols = cat_cols if cat_cols else [] # categorical inputs are stored in \"self.cat_cols\"\n",
    "        self.cont_cols = [\n",
    "            col for col in data.columns if col not in self.cat_cols + [output_col] # finds out the columns that have \n",
    "                                                                                   # contionuos inputs\n",
    "        ]\n",
    "\n",
    "        if self.cont_cols:\n",
    "            self.cont_X = data[self.cont_cols].astype(np.float32).values # The data corresponding to \n",
    "                                                                         # continuous columns are stored in \"self.cont_x\"\n",
    "        else:\n",
    "            self.cont_X = np.zeros((self.n, 1))\n",
    "\n",
    "        if self.cat_cols:\n",
    "            self.cat_X = data[cat_cols].astype(np.int64).values # The data corresponding to \n",
    "                                                                # categorical columns are stored in \"self.cont_x\"\n",
    "        else:\n",
    "            self.cat_X = np.zeros((self.n, 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Denotes the total number of samples.\n",
    "        \"\"\"\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Generates one sample of data as a triplet --> #(output, continous inputs, categorical inputs)\n",
    "        \"\"\"\n",
    "        return [self.y[idx], self.cont_X[idx], self.cat_X[idx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of the Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_dims,\n",
    "        no_of_cont,\n",
    "        lin_layer_sizes,\n",
    "        output_size,\n",
    "        emb_dropout,\n",
    "        lin_layer_dropouts,\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        emb_dims: List of two element tuples\n",
    "        This list will contain a two element tuple for each\n",
    "        categorical feature. The first element of a tuple will\n",
    "        denote the number of unique values of the categorical\n",
    "        feature. The second element will denote the embedding\n",
    "        dimension to be used for that feature.\n",
    "\n",
    "        no_of_cont: Integer\n",
    "        The number of continuous features in the data.\n",
    "\n",
    "        lin_layer_sizes: List of integers.\n",
    "        The size of each linear layer. The length will be equal\n",
    "        to the total number\n",
    "        of linear layers in the network.\n",
    "\n",
    "        output_size: Integer\n",
    "        The size of the final output.\n",
    "\n",
    "        emb_dropout: Float\n",
    "        The dropout to be used after the embedding layers.\n",
    "\n",
    "        lin_layer_dropouts: List of floats\n",
    "        The dropouts to be used after each linear layer.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layers of categorical inputs\n",
    "        self.emb_layers = nn.ModuleList([nn.Embedding(x, y) for x, y in emb_dims])\n",
    "\n",
    "        no_of_embs = sum([y for x, y in emb_dims])\n",
    "        self.no_of_embs = no_of_embs\n",
    "        self.no_of_cont = no_of_cont\n",
    "\n",
    "        # Linear Layers of the nueral network\n",
    "        first_lin_layer = nn.Linear(\n",
    "            self.no_of_embs + self.no_of_cont, lin_layer_sizes[0]\n",
    "        ) # first layer where data is fed into the network\n",
    "        # Following lines define the hidden layers\n",
    "        self.lin_layers = nn.ModuleList(\n",
    "            [first_lin_layer]\n",
    "            + [\n",
    "                nn.Linear(lin_layer_sizes[i], lin_layer_sizes[i + 1])\n",
    "                for i in range(len(lin_layer_sizes) - 1)\n",
    "            ]\n",
    "        )\n",
    "        # Initializing the weights of neural network layers\n",
    "        for lin_layer in self.lin_layers:\n",
    "            nn.init.kaiming_normal_(lin_layer.weight.data)\n",
    "\n",
    "        # Output Layer\n",
    "        self.output_layer = nn.Linear(lin_layer_sizes[-1], output_size)\n",
    "        nn.init.kaiming_normal_(self.output_layer.weight.data)\n",
    "\n",
    "        # Batch Norm Layers, It is used to normalize the data into the layers to avoid overfitting and to optimize network\n",
    "        self.first_bn_layer = nn.BatchNorm1d(self.no_of_cont)\n",
    "        self.bn_layers = nn.ModuleList(\n",
    "            [nn.BatchNorm1d(size) for size in lin_layer_sizes]\n",
    "        )\n",
    "\n",
    "        # Dropout Layers used as regularization\n",
    "        self.emb_dropout_layer = nn.Dropout(emb_dropout)\n",
    "        self.droput_layers = nn.ModuleList(\n",
    "            [nn.Dropout(size) for size in lin_layer_dropouts]\n",
    "        )\n",
    "\n",
    "    def forward(self, cont_data, cat_data):\n",
    "        # This function defines the mathematical operations of the neural network \n",
    "        if self.no_of_embs != 0: # Defines the embedding layer of categorical inputs\n",
    "            x = [\n",
    "                emb_layer(cat_data[:, i]) for i, emb_layer in enumerate(self.emb_layers)\n",
    "            ]\n",
    "            x = torch.cat(x, 1)\n",
    "            x = self.emb_dropout_layer(x) # dropout regularization of categorical inputs \n",
    "\n",
    "        if self.no_of_cont != 0:\n",
    "            normalized_cont_data = self.first_bn_layer(cont_data) # defines the linear (normal nueral net hidden layer)\n",
    "                                                                  # for continous inputs\n",
    "\n",
    "            if self.no_of_embs != 0:\n",
    "                x = torch.cat([x, normalized_cont_data], 1)\n",
    "            else:\n",
    "                x = normalized_cont_data\n",
    "        # Batch normalization operation is done in this for loop\n",
    "        for lin_layer, dropout_layer, bn_layer in zip( \n",
    "            self.lin_layers, self.droput_layers, self.bn_layers\n",
    "        ):\n",
    "\n",
    "            x = F.relu(lin_layer(x))\n",
    "#             x = F.sigmoid(lin_layer(x))\n",
    "            x = bn_layer(x)\n",
    "            x = dropout_layer(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x # Returns the output of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is read as a pandas data frame\n",
    "data = pd.read_csv(\"finalized_data.csv\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Array that specifies which columns are categorical inputs\n",
    "categorical_features = [\"0\", \"1\", \"2\", \"3\", \"4\",\"5\", \"6\", \"7\", \"8\"] # Except column \"7\", all are label encoded and \"7\" is \n",
    "                                                               # one-hot encoded. \n",
    "# Array that specifies which columns are continuous inputs\n",
    "contnuous_features = [\"9\",\"10\",\"11\"]\n",
    "output_feature = \"12\"  # specifies which column is output\n",
    "\n",
    "\n",
    "temp={}\n",
    "label_encoders = {} \n",
    "# for loop starts for label encoding\n",
    "for i in range(0,len(categorical_features)):\n",
    "    #codes for one-hot encoding\n",
    "    label_encoders[categorical_features[i]] = OneHotEncoder(handle_unknown='ignore')\n",
    "    label_encoders[categorical_features[i]].fit(data[categorical_features[i]].values.reshape(-1,1))\n",
    "    temp[categorical_features[i]]=label_encoders[categorical_features[i]].transform(data[categorical_features[i]].values.reshape(-1,1)).toarray() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1263600, 13)\n",
      "(1263600, 3)\n",
      "(1263600, 3)\n",
      "(1263600, 3)\n",
      "(1263600, 3)\n",
      "(1263600, 4)\n",
      "(1263600, 3)\n",
      "(1263600, 4)\n",
      "(1263600, 21)\n"
     ]
    }
   ],
   "source": [
    "#printing the dimensions of each categorical variables after one-hot encoding\n",
    "for i in range(0,len(temp)):\n",
    "    print(temp[categorical_features[i]].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the one hot encoded features into the data\n",
    "# The new columns will be named from 13 onwards.\n",
    "ind=13\n",
    "new_categorical_features=[]\n",
    "for i in range(0,len(categorical_features)):\n",
    "    for j in range(0,temp[categorical_features[i]].shape[1]):\n",
    "        data[str(ind)] = temp[categorical_features[i]][:,j]\n",
    "        new_categorical_features.append(str(ind)) # Stroing the column identity of newly added one-hot encoded columns as \n",
    "                                                  # categorical input.\n",
    "        ind=ind+1\n",
    "    data=data.drop([categorical_features[i]], axis=1) # The old column is deleted from the data\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '20',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '30',\n",
       " '31',\n",
       " '32',\n",
       " '33',\n",
       " '34',\n",
       " '35',\n",
       " '36',\n",
       " '37',\n",
       " '38',\n",
       " '39',\n",
       " '40',\n",
       " '41',\n",
       " '42',\n",
       " '43',\n",
       " '44',\n",
       " '45',\n",
       " '46',\n",
       " '47',\n",
       " '48',\n",
       " '49',\n",
       " '50',\n",
       " '51',\n",
       " '52',\n",
       " '53',\n",
       " '54',\n",
       " '55',\n",
       " '56',\n",
       " '57',\n",
       " '58',\n",
       " '59',\n",
       " '60',\n",
       " '61',\n",
       " '62',\n",
       " '63',\n",
       " '64',\n",
       " '65',\n",
       " '66',\n",
       " '67',\n",
       " '68',\n",
       " '69']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 9         10         11        12   13   14   15   16   17  \\\n",
      "0        -1.000000 -13.016747   6.399998  0.010556  1.0  0.0  0.0  0.0  0.0   \n",
      "1        -1.000000 -13.016747   7.199998  0.011241  1.0  0.0  0.0  0.0  0.0   \n",
      "2        -1.000000 -13.016747   7.999998  0.012185  1.0  0.0  0.0  0.0  0.0   \n",
      "3        -1.000000 -13.016747   8.799997  0.013634  1.0  0.0  0.0  0.0  0.0   \n",
      "4        -1.000000 -13.016747   9.599998  0.014240  1.0  0.0  0.0  0.0  0.0   \n",
      "...            ...        ...        ...       ...  ...  ...  ...  ...  ...   \n",
      "1263595  12.283293  -3.000003  66.399963  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1263596  12.283293  -3.000003  67.199959  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1263597  12.283293  -3.000003  67.999962  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1263598  12.283293  -3.000003  68.799957  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1263599  12.283293  -3.000003  69.599960  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "          18  ...   60   61   62   63   64   65   66   67   68   69  \n",
      "0        0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1        0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2        0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3        0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4        0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "1263595  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "1263596  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "1263597  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "1263598  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "1263599  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "\n",
      "[1263600 rows x 61 columns]\n",
      "(1263600, 61)\n"
     ]
    }
   ],
   "source": [
    "print(data) # Data after one-hot encoding.\n",
    "print(data.shape)     # New dimension of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20',\n",
       "       '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32',\n",
       "       '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44',\n",
       "       '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56',\n",
       "       '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68',\n",
       "       '69'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'index.csv'\n",
    "raw_data = open(filename, 'rt')\n",
    "reader = csv.reader(raw_data, delimiter=',', quoting=csv.QUOTE_NONE)\n",
    "x = list(reader)\n",
    "index = np.array(x).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.to_numpy() \n",
    "instances=X.shape[0]/25\n",
    "# index = np.random.permutation(int(instances))\n",
    "train=np.zeros((int(np.floor(0.8*instances))*25, X.shape[1]))\n",
    "print(train.shape)\n",
    " \n",
    "\n",
    "for i in range(int(np.floor(0.8*instances))):\n",
    "    t1=i*25\n",
    "    t2=int(index[i]*25)\n",
    "    train[t1:t1+25,:] = X[t2:t2+25,:]\n",
    " \n",
    "\n",
    "print(train.shape)\n",
    "t4=int(instances)-int(np.floor(0.8*instances))\n",
    "test_new=np.zeros((t4*25, data.shape[1]))\n",
    " \n",
    "t3=int(np.floor(0.8*instances))\n",
    "\n",
    "for i in range(t4):\n",
    "    t1=i*25\n",
    "    t2=int(index[i+t3]*25)\n",
    "    test_new[t1:t1+25,:] = X[t2:t2+25,:]\n",
    " \n",
    "print(test_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_permuted=np.zeros((train.shape[0], train.shape[1]))\n",
    "\n",
    "# instances1=train.shape[0]\n",
    "# index1 = np.random.permutation(int(instances1))\n",
    "# for i in range(instances1):\n",
    "#     train_permuted[i,:]=train[int(index1[i]),:]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training (80%) and testing (20%)\n",
    "# train_all, test = train_test_split(data, test_size=0.2, shuffle=True) \n",
    "# Splitting the training data into training set (7/8 portion) into training set and validation set (1/8 portion)\n",
    "val, test = train_test_split(test_new, test_size=2/3, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "# print(val.shape)\n",
    "print(test_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_model(arch, emb_dims, dropout_rate, no_of_epochs, criterion, lr, traindataloader, valdataloader, testdataloader, ground_label, is_test):\n",
    "    device = torch.device('cuda')\n",
    "    model = FeedForwardNN(emb_dims, no_of_cont=3, lin_layer_sizes=arch, output_size=1, emb_dropout=dropout_rate, \n",
    "                          lin_layer_dropouts=[dropout_rate] * len(arch)).to(device)\n",
    "    \n",
    "    no_of_epochs = no_of_epochs # Number of times the data is fed into the network \n",
    "    criterion = criterion # Objective function of the network\n",
    "    lr=lr\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr) # Optimization algorithm to be used\n",
    "    stopper=10 # A value to check overfitting\n",
    "    loss_train_prev=0 # VAriable to store value of objective function in the previous epoch\n",
    "    Ltrain=[] # Python list to store training loss in each epoch\n",
    "    Ltest=[] # Python list to store testing loss in each epoch\n",
    "    \n",
    "    \n",
    "    # Neural network training starts from here\n",
    "    for epoch in range(no_of_epochs):\n",
    "      loss_var=[]  \n",
    "      for y, cont_x, cat_x in traindataloader: # Loading the training dataset\n",
    "          \n",
    "        cat_x = cat_x.to(device)\n",
    "        cont_x = cont_x.to(device)\n",
    "        y  = y.to(device)\n",
    "\n",
    "        # Forward Pass\n",
    "        preds = model(cont_x, cat_x)\n",
    "        loss = criterion(preds, y)\n",
    "        loss_var.append(loss.item())\n",
    "#         print(loss_var)\n",
    "#         print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "        \n",
    "        # Backward Pass and Optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#       print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "      loss_train=sum(loss_var)/(train.shape[0]/batchsize_train) # Finding out the objective function\n",
    "       \n",
    "    \n",
    "      Ltrain.append(loss_train)\n",
    "      \n",
    "       \n",
    "    \n",
    "      # Doing validation of the Neural network\n",
    "      loss_var=[]  \n",
    "      for y, cont_x, cat_x in valdataloader:\n",
    "        \n",
    "        cat_x = cat_x.to(device)\n",
    "        cont_x = cont_x.to(device)\n",
    "        y  = y.to(device)\n",
    "\n",
    "        # Forward Pass\n",
    "        preds = model(cont_x, cat_x)\n",
    "        loss = criterion(preds, y)\n",
    "        loss_var.append(loss.item())\n",
    "      loss_test=sum(loss_var)/(val.shape[0]/batchsize_val)  # Finding out the objective function\n",
    "       \n",
    "      Ltest.append(loss_test)\n",
    "#         print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "        \n",
    "        # Backward Pass and Optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "      if is_test==False:\n",
    "          if epoch % 20==0:\n",
    "              print('Epoch {}: train loss: {} validation loss: {}'.format(epoch, loss_train, loss_test))\n",
    "      else:\n",
    "          print('Epoch {}: train loss: {} validation loss: {}'.format(epoch, loss_train, loss_test))\n",
    "              \n",
    "      if epoch > stopper and Ltest[-1] > np.mean(Ltest[-(stopper+1):-1]):  # Checking overfitting \n",
    "            print(\"Early Stoppin.....\")\n",
    "            break\n",
    "            \n",
    "    # Testing\n",
    "    predicted=np.zeros((0,1))  # Variable to store neural network prediction\n",
    "    for y, cont_x, cat_x in testdataloader:\n",
    "        cat_x = cat_x.to(device)\n",
    "        cont_x = cont_x.to(device)\n",
    "#         y  = y.to(device)\n",
    "        preds = model(cont_x, cat_x)\n",
    "        preds=preds.cpu()\n",
    "        predicted = np.concatenate((predicted, preds.detach().numpy()),axis=0)\n",
    "    actual = ground_label # Variable storing the actual values of the testing cases\n",
    "    actual=actual.reshape(actual.shape[0],1) # Reshaping for visualization\n",
    "    result=np.concatenate((actual, predicted),axis=1) # Actual value and predicted value is stored in \"result\" variable.\n",
    "    \n",
    "    MSE = mean_squared_error(actual, predicted)\n",
    "    MAE = mean_absolute_error(actual, predicted)\n",
    "    R = r2_score(actual, predicted)\n",
    "    \n",
    "    if is_test:\n",
    "        np.savetxt('NN_result.csv', result, delimiter=',')\n",
    "    print('MSE: {} MAE: {} R2: {}'.format(MSE, MAE, R))\n",
    "    return MSE, MAE, R, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "kf = KFold(n_splits)\n",
    "kf.get_n_splits(train) \n",
    "KFold(n_splits=5, random_state=None, shuffle=False)\n",
    "architectures=[ [64], [128], [256], [512], [1024], [64,64], [128,128], [256,256], [512,512], [1024,1024], \n",
    "               [64,64,64], [128,128,128], [256,256,256], [512,512,512], [1024,1024,1024], \n",
    "              [64,64,64,64], [128,128,128,128], [256,256,256,256], [512,512,512,512], [1024,1024,1024,1024],\n",
    "              ]\n",
    "Results = np.zeros((len(architectures), n_splits,3))\n",
    "cat_dims = [2 for col in new_categorical_features]\n",
    "    \n",
    "# Finding out the embedding dimensions for the categorical features\n",
    "emb_dims = [(x, min(50, (x + 1) // 2)) for x in cat_dims]\n",
    "dropout_rate = 0.5\n",
    "no_of_epochs = 100\n",
    "criterion = nn.MSELoss()\n",
    "lr = 0.01\n",
    "k=0\n",
    "for arch in architectures:\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(arch)\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    p=0\n",
    "    for train_index, test_index in kf.split(train):\n",
    "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = train[train_index], train[test_index]\n",
    "        val_k, test_k = train_test_split(X_test, test_size=2/3, shuffle = False)\n",
    "    \n",
    "        train_k=pd.DataFrame(X_train)\n",
    "        val_k=pd.DataFrame(val_k)\n",
    "        test_k=pd.DataFrame(test_k)\n",
    "\n",
    "        train_columns=[]\n",
    "        for i in range(0,61):\n",
    "            train_columns.append(str(i))\n",
    "        train_k.columns=train_columns\n",
    "        test_k.columns=train_columns\n",
    "        val_k.columns=train_columns\n",
    "\n",
    "        new_categorical_features=train_columns[4:]\n",
    "        output_feature=train_columns[3]\n",
    "     \n",
    "        # Creating the training dataset to feed into the network\n",
    "        traindataset = TabularDataset(data=train_k, cat_cols=new_categorical_features,\n",
    "                             output_col=output_feature)\n",
    "        # Creating the validation dataset to feed into the network\n",
    "        valdataset = TabularDataset(data=val_k, cat_cols=new_categorical_features,\n",
    "                             output_col=output_feature)\n",
    "        # Creating the testing dataset to feed into the network\n",
    "        testdataset = TabularDataset(data=test_k, cat_cols=new_categorical_features,\n",
    "                             output_col=output_feature)\n",
    "    \n",
    "        batchsize_train = 16000\n",
    "        batchsize_val=1381\n",
    "        batchsize_test=42121\n",
    "        # Defining pytorch dataloader for train, validation and test dataset\n",
    "        traindataloader = DataLoader(traindataset, batchsize_train, shuffle=True, num_workers=0, ) \n",
    "        valdataloader = DataLoader(valdataset, batchsize_val, shuffle=False, num_workers=0)\n",
    "        testdataloader = DataLoader(testdataset, batchsize_test, shuffle=False, num_workers=0)\n",
    "    \n",
    "        # Finding out the uniue values corresponding to the categoricl features\n",
    "        \n",
    "        MSE, MAE, R, predicted = NN_model(arch, emb_dims, dropout_rate, no_of_epochs, criterion, lr, traindataloader, \n",
    "                               valdataloader, testdataloader, test_k['3'].to_numpy(), is_test = False)\n",
    "         #arch, emb_dims, dropout_rate, no_of_epochs, criterion, lr, traindataloader, valdataloader, testdataloader, \n",
    "            #ground_label, is_test):\n",
    "   \n",
    "        Results[k,p,0] = MSE\n",
    "        Results[k,p,1] = MAE\n",
    "        Results[k,p,2] = R\n",
    "        p+=1\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "\n",
    "# total time taken\n",
    "print(f\"Runtime of the program is {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('Cross_validation_MSE_results.csv', Results[:,:,0], delimiter=',')\n",
    "np.savetxt('Cross_validation_MAE_results.csv', Results[:,:,1], delimiter=',')\n",
    "np.savetxt('Cross_validation_R2_results.csv', Results[:,:,2], delimiter=',')\n",
    "RMSE_summary = Results[:,:,0]\n",
    "RMSE_summary = np.sum(RMSE_summary, axis = 1)\n",
    "RMSE_summary = np.argmin(RMSE_summary)\n",
    " \n",
    "arch = architectures[RMSE_summary]\n",
    "print(\"Architecture selected through cross validation:\")\n",
    "print(arch)\n",
    "train=pd.DataFrame(train)\n",
    "val=pd.DataFrame(val)\n",
    "test=pd.DataFrame(test)\n",
    "\n",
    "train_columns=[]\n",
    "for i in range(0,61):\n",
    "    train_columns.append(str(i))\n",
    "train.columns=train_columns\n",
    "test.columns=train_columns\n",
    "val.columns=train_columns\n",
    "\n",
    "new_categorical_features=train_columns[4:]\n",
    "output_feature=train_columns[3]\n",
    "     \n",
    "# Creating the training dataset to feed into the network\n",
    "traindataset = TabularDataset(data=train, cat_cols=new_categorical_features,\n",
    "                             output_col=output_feature)\n",
    "# Creating the validation dataset to feed into the network\n",
    "valdataset = TabularDataset(data=val, cat_cols=new_categorical_features,\n",
    "                             output_col=output_feature)\n",
    "# Creating the testing dataset to feed into the network\n",
    "testdataset = TabularDataset(data=test, cat_cols=new_categorical_features,\n",
    "                             output_col=output_feature)\n",
    "    \n",
    "batchsize_train = 16000\n",
    "batchsize_val=1381\n",
    "batchsize_test=42121\n",
    "# Defining pytorch dataloader for train, validation and test dataset\n",
    "traindataloader = DataLoader(traindataset, batchsize_train, shuffle=True, num_workers=0) \n",
    "valdataloader = DataLoader(valdataset, batchsize_val, shuffle=False, num_workers=0)\n",
    "testdataloader = DataLoader(testdataset, batchsize_test, shuffle=False, num_workers=0)\n",
    "    \n",
    "# Finding out the uniue values corresponding to the categoricl features\n",
    "cat_dims = [2 for col in new_categorical_features]\n",
    "    \n",
    "# Finding out the embedding dimensions for the categorical features\n",
    "emb_dims = [(x, min(50, (x + 1) // 2)) for x in cat_dims]\n",
    "dropout_rate = 0.5\n",
    "no_of_epochs = 100\n",
    "criterion = nn.MSELoss()\n",
    "lr = 0.01\n",
    "actual= test['3'].to_numpy()\n",
    "MSE, MAE, R, predicted = NN_model(arch, emb_dims, dropout_rate, no_of_epochs, criterion, lr, traindataloader, valdataloader, testdataloader, actual,is_test = True)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean suared error of the prediction is printed\n",
    "\n",
    "print(\"Mean squared error is:\")\n",
    "print(MSE)\n",
    "print(\"Mean absolute error is:\")\n",
    "print(MAE)\n",
    "print(\"R2 score is:\")\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actual =  test['3'].to_numpy()\n",
    "actual = np.reshape(actual,(actual.shape[0],1))\n",
    "real=np.zeros((25,1))\n",
    "predctd = np.zeros((25,1))\n",
    "\n",
    "pos = int(input(\"Enter an integer between 0 to 6738 to test the model : \"))\n",
    "pos = pos*25+9\n",
    "\n",
    "real=actual[pos:pos+25,-1]\n",
    "predctd = predicted[pos:pos+25,-1]\n",
    "\n",
    "real=np.reshape(real,(5,5))\n",
    "predctd = np.reshape(predctd,(5,5))\n",
    "\n",
    "\n",
    "print(\"Heatmap Actual and Heatmap predicted\")\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "heat_map = sb.heatmap(real, vmin = 0, vmax = 1.30141, cmap=\"OrRd_r\", annot=True)\n",
    "plt.subplot(122)\n",
    "heat_map = sb.heatmap(predctd,  vmin = 0, vmax = 1.30141, cmap=\"OrRd_r\", annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
